\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand*\new@tpo@label[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Networks}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Notation}{3}{}\protected@file@percent }
\newlabel{sec:notation}{{2.1}{3}}
\newlabel{def:layer}{{2.2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of a neuron as graph.}}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of a layer as graph.}}{3}{}\protected@file@percent }
\newlabel{def:network}{{2.3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of a neural network as graph.}}{4}{}\protected@file@percent }
\newlabel{fig:neuron}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Training in the Parameter Space}{5}{}\protected@file@percent }
\newlabel{sec:Training}{{2.2}{5}}
\citation{GD}
\citation{ConvexOptimization}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Descent}}{6}{}\protected@file@percent }
\newlabel{lem:descent}{{2.8}{7}}
\citation{ConvexOptimization}
\newlabel{thm:descent}{{2.9}{8}}
\citation{BFGS}
\newlabel{lem:convexity}{{2.12}{9}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Broyden-Fletcher-Goldfarb-Shanno (BFGS)}}{10}{}\protected@file@percent }
\citation{SGD}
\citation{Adam}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Stochastic Gradient Descent (SGD)}}{11}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Adaptive Moment Estimation (Adam)}}{12}{}\protected@file@percent }
\citation{NTK}
\@writefile{toc}{\contentsline {section}{\numberline {3}Training in the Function Space}{13}{}\protected@file@percent }
\newlabel{sec:functionspace}{{3}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Motivation}{13}{}\protected@file@percent }
\citation{Functionals}
\citation{NTK}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Functional Derivative}{14}{}\protected@file@percent }
\citation{FunctionalAnalysis}
\citation{FunctionalAnalysis}
\newlabel{lem:projection}{{3.6}{18}}
\citation{FunctionalAnalysis}
\newlabel{cor:projection}{{3.7}{19}}
\newlabel{thm:riesz}{{3.8}{19}}
\citation{NTK}
\newlabel{lem:funcDerivative}{{3.9}{20}}
\citation{NTK}
\citation{RKHS}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Reproducing Kernel Hilbert Space}{22}{}\protected@file@percent }
\newlabel{thm:rkhs}{{3.12}{22}}
\citation{NTK}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Kernel Gradient Descent}{25}{}\protected@file@percent }
\newlabel{sec:KGD}{{3.4}{25}}
\newlabel{def:phi}{{3.15}{25}}
\newlabel{lem:phi}{{3.16}{26}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Kernel Gradient Descent}}{27}{}\protected@file@percent }
\newlabel{lem:form}{{3.19}{28}}
\citation{Functionals}
\citation{Features}
\citation{NTK}
\newlabel{def:definite}{{3.21}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Neural Tangent Kernel}{31}{}\protected@file@percent }
\newlabel{def:follow}{{3.22}{31}}
\citation{Functionals}
\newlabel{lem:evolution}{{3.25}{32}}
\newlabel{lem:tk}{{3.26}{33}}
\citation{NTK}
\newlabel{def:ntk}{{3.27}{34}}
\citation{NTK}
\citation{NTK}
\citation{NTK}
\citation{Paper}
\citation{Paper}
\@writefile{toc}{\contentsline {section}{\numberline {4}Training in Low-Dimensional Subspaces}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Approach}{37}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Low-dimensional parameter trajectory}}{37}{}\protected@file@percent }
\newlabel{fig:trajectory}{{4}{37}}
\citation{SVD}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Theory}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Singular Value Decomposition}{38}{}\protected@file@percent }
\newlabel{thm:svd}{{4.1}{38}}
\newlabel{cor:svd}{{4.3}{38}}
\citation{SVD}
\newlabel{thm:eym}{{4.5}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Dynamic Linear Dimensionality Reduction}{39}{}\protected@file@percent }
\newlabel{def:flow}{{4.6}{40}}
\newlabel{lem:flow}{{4.7}{40}}
\citation{SVD}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Methodology}{43}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Orthogonal Projections}{43}{}\protected@file@percent }
\newlabel{lem:orthProjection}{{4.8}{43}}
\citation{PCA}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Principal Component Analysis}{44}{}\protected@file@percent }
\newlabel{sec:PCA}{{4.3.2}{44}}
\newlabel{thm:PCA}{{4.10}{45}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Algorithm}{46}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Dynamic Linear Dimensionality Reduction (DLDR)}}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{49}{}\protected@file@percent }
\bibcite{Paper}{1}
\bibcite{GD}{2}
\bibcite{ConvexOptimization}{3}
\bibcite{BFGS}{4}
\bibcite{SGD}{5}
\bibcite{Adam}{6}
\bibcite{NTK}{7}
\bibcite{Functionals}{8}
\bibcite{FunctionalAnalysis}{9}
\bibcite{RKHS}{10}
\bibcite{Features}{11}
\bibcite{Stieltjes}{12}
\bibcite{Linear}{13}
\bibcite{SVD}{14}
\bibcite{PCA}{15}
\gdef \@abspage@last{52}
