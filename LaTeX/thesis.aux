\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand*\new@tpo@label[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Networks}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Notation}{3}{}\protected@file@percent }
\newlabel{sec:notation}{{2.1}{3}}
\newlabel{def:layer}{{2.2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of a neuron as graph.}}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of a layer as graph.}}{3}{}\protected@file@percent }
\newlabel{def:network}{{2.3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of a neural network as graph.}}{4}{}\protected@file@percent }
\newlabel{fig:neuron}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Training in the Parameter Space}{5}{}\protected@file@percent }
\newlabel{sec:Training}{{2.2}{5}}
\citation{GD}
\citation{ConvexOptimization}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Descent}}{6}{}\protected@file@percent }
\newlabel{lem:descent}{{2.8}{7}}
\citation{ConvexOptimization}
\newlabel{thm:descent}{{2.9}{8}}
\citation{BFGS}
\newlabel{lem:convexity}{{2.12}{9}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Broyden-Fletcher-Goldfarb-Shanno (BFGS)}}{10}{}\protected@file@percent }
\citation{SGD}
\citation{Adam}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Stochastic Gradient Descent (SGD)}}{11}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Adaptive Moment Estimation (Adam)}}{12}{}\protected@file@percent }
\citation{NTK}
\citation{Functionals}
\@writefile{toc}{\contentsline {section}{\numberline {3}Training in the Function Space}{13}{}\protected@file@percent }
\newlabel{sec:kernel}{{3}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Functional Gradient Descent}{13}{}\protected@file@percent }
\newlabel{def:derivative}{{3.2}{14}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Functional Gradient Descent}}{14}{}\protected@file@percent }
\citation{NTK}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Dual Space}{16}{}\protected@file@percent }
\citation{FunctionalAnalysis}
\newlabel{lem:projection}{{3.6}{18}}
\citation{FunctionalAnalysis}
\citation{FunctionalAnalysis}
\newlabel{cor:projection}{{3.7}{19}}
\newlabel{lem:riesz}{{3.8}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Reproducing Kernel Hilbert Space}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Kernel Gradient Descent}{24}{}\protected@file@percent }
\newlabel{lem:form}{{3.16}{24}}
\newlabel{def:definite}{{3.17}{25}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Kernel Gradient Descent}}{26}{}\protected@file@percent }
\newlabel{lem:phi}{{3.22}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Kernel Approximation}{29}{}\protected@file@percent }
\newlabel{sec:approximation}{{3.5}{29}}
\newlabel{def:follow}{{3.27}{29}}
\newlabel{lem:evolution}{{3.28}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Neural Tangent Kernel}{31}{}\protected@file@percent }
\newlabel{def:ntk}{{3.30}{31}}
\citation{Stieltjes}
\citation{Linear}
\newlabel{thm:stieltjes}{{3.35}{32}}
\newlabel{thm:linear}{{3.36}{32}}
\citation{Paper}
\citation{Paper}
\@writefile{toc}{\contentsline {section}{\numberline {4}Training in Low-Dimensional Subspaces}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Approach}{34}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Low-dimensional parameter trajectory}}{34}{}\protected@file@percent }
\newlabel{fig:trajectory}{{4}{34}}
\citation{SVD}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Theory}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Singular Value Decomposition}{35}{}\protected@file@percent }
\newlabel{thm:svd}{{4.1}{35}}
\newlabel{cor:svd}{{4.3}{35}}
\citation{SVD}
\newlabel{thm:eym}{{4.5}{36}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Dynamic Linear Dimensionality Reduction}{36}{}\protected@file@percent }
\newlabel{def:flow}{{4.6}{37}}
\newlabel{lem:flow}{{4.7}{37}}
\citation{SVD}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Methodology}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Orthogonal Projections}{40}{}\protected@file@percent }
\newlabel{lem:orthProjection}{{4.8}{40}}
\citation{PCA}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Principal Component Analysis}{41}{}\protected@file@percent }
\newlabel{sec:PCA}{{4.3.2}{41}}
\newlabel{thm:PCA}{{4.10}{42}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Algorithm}{43}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Dynamic Linear Dimensionality Reduction (DLDR)}}{43}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{45}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Appendix}{46}{}\protected@file@percent }
\newlabel{cor:descent}{{7.3}{46}}
\newlabel{lem:coercivity}{{7.4}{46}}
\bibcite{Paper}{1}
\bibcite{GD}{2}
\bibcite{ConvexOptimization}{3}
\bibcite{BFGS}{4}
\bibcite{SGD}{5}
\bibcite{Adam}{6}
\bibcite{NTK}{7}
\bibcite{Functionals}{8}
\bibcite{FunctionalAnalysis}{9}
\bibcite{Stieltjes}{10}
\bibcite{Linear}{11}
\bibcite{SVD}{12}
\bibcite{PCA}{13}
\gdef \@abspage@last{49}
