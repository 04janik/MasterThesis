\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand*\new@tpo@label[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Networks}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Notation}{3}{}\protected@file@percent }
\newlabel{sec:notation}{{2.1}{3}}
\newlabel{def:layer}{{2.2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of a neuron as graph.}}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of a layer as graph.}}{3}{}\protected@file@percent }
\newlabel{def:network}{{2.3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of a neural network as graph.}}{4}{}\protected@file@percent }
\newlabel{fig:neuron}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Training}{5}{}\protected@file@percent }
\newlabel{sec:Training}{{2.2}{5}}
\citation{GD}
\citation{ConvexOptimization}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Descent}}{6}{}\protected@file@percent }
\newlabel{lem:descent}{{2.8}{7}}
\citation{ConvexOptimization}
\newlabel{thm:descent}{{2.9}{8}}
\citation{BFGS}
\newlabel{lem:convexity}{{2.12}{9}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Broyden-Fletcher-Goldfarb-Shanno (BFGS)}}{10}{}\protected@file@percent }
\citation{SGD}
\citation{Adam}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Stochastic Gradient Descent (SGD)}}{11}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Adaptive Moment Estimation (Adam)}}{12}{}\protected@file@percent }
\citation{NTK}
\@writefile{toc}{\contentsline {section}{\numberline {3}Kernel Gradient Descent}{15}{}\protected@file@percent }
\newlabel{sec:kernel}{{3}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Functional Derivatives}{15}{}\protected@file@percent }
\newlabel{lem:projection}{{3.4}{17}}
\newlabel{cor:projection}{{3.5}{18}}
\newlabel{lem:riesz}{{3.6}{18}}
\newlabel{def:derivative}{{3.7}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Kernel Gradient}{20}{}\protected@file@percent }
\newlabel{lem:form}{{3.10}{20}}
\newlabel{lem:phi}{{3.16}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Kernel Approximation}{25}{}\protected@file@percent }
\newlabel{sec:approximation}{{3.3}{25}}
\newlabel{def:follow}{{3.21}{25}}
\newlabel{lem:evolution}{{3.22}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Neural Tangent Kernel}{27}{}\protected@file@percent }
\newlabel{def:ntk}{{3.24}{27}}
\citation{Stieltjes}
\citation{Linear}
\newlabel{thm:stieltjes}{{3.29}{28}}
\newlabel{thm:linear}{{3.30}{28}}
\citation{Paper}
\citation{Paper}
\@writefile{toc}{\contentsline {section}{\numberline {4}Dynamic Linear Dimensionality Reduction}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Approach}{30}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Low-dimensional parameter trajectory}}{30}{}\protected@file@percent }
\newlabel{fig:trajectory}{{4}{30}}
\citation{SVD}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Theory}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Singular Value Decomposition}{31}{}\protected@file@percent }
\newlabel{thm:svd}{{4.1}{31}}
\newlabel{cor:svd}{{4.3}{31}}
\citation{SVD}
\newlabel{thm:eym}{{4.5}{32}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Dimensionality Reduction}{32}{}\protected@file@percent }
\newlabel{def:flow}{{4.6}{33}}
\newlabel{lem:flow}{{4.7}{33}}
\citation{SVD}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Methodology}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Orthogonal Projections}{36}{}\protected@file@percent }
\newlabel{lem:orthProjection}{{4.8}{36}}
\citation{PCA}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Principal Component Analysis}{37}{}\protected@file@percent }
\newlabel{sec:PCA}{{4.3.2}{37}}
\newlabel{thm:PCA}{{4.10}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Algorithm}{39}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Dynamic Linear Dimensionality Reduction (DLDR)}}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Appendix}{42}{}\protected@file@percent }
\newlabel{cor:descent}{{7.3}{42}}
\newlabel{lem:coercivity}{{7.4}{42}}
\bibcite{Paper}{1}
\bibcite{GD}{2}
\bibcite{ConvexOptimization}{3}
\bibcite{BFGS}{4}
\bibcite{SGD}{5}
\bibcite{Adam}{6}
\bibcite{NTK}{7}
\bibcite{Stieltjes}{8}
\bibcite{Linear}{9}
\bibcite{SVD}{10}
\bibcite{PCA}{11}
\gdef \@abspage@last{45}
