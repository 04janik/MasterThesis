
\documentclass[11pt, a4paper]{article}
\usepackage[left= 2.5cm, right = 3cm, bottom = 4 cm]{geometry}
\usepackage[english]{babel}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[R]{\thepage}
\fancyhead[L]{\nouppercase{\leftmark}}
\fancyfoot{}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{algorithm}[theorem]{Algorithm}

\usepackage{amssymb}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}	
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\renewcommand{\H}{\mathcal{H}}
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\P}{\mathcal{P}}

\renewcommand{\b}{\mathsf{b}}
\newcommand{\w}{\mathsf{w}}
\newcommand*{\tr}{^{\mkern-1.5mu\mathsf{T}}}

\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\E}{\mathbb{E}}

\usepackage{graphicx, scrextend, algorithmic, xcolor, mathabx}

\usepackage[ruled]{algorithm}

\setlength{\parindent}{0em} 

%%%%%%%%%%

\begin{document}

\tableofcontents
\thispagestyle{empty}

\pagebreak
\section{Introduction}

This thesis is about...

\pagebreak
\section{Neural Networks}

This section introduces the basic concepts of neural networks and provides the necessary notation for further sections. We start with the formal definition of neural networks and explain the process of training afterwards. Finally we will introduce several training methods.

\subsection{Notation} \label{sec:notation}

To start things off, we consider the simple model of a neuron. 

\begin{definition}
Let $n_x \in \N$, $\w \in \R^{n_x}$, $\b \in \R$ and $\sigma: \R \to \R$. A neuron is defined as
\[ \nu : \R^{n_x} \to \R : x \mapsto \sigma \Big ( \w \tr x + \b \Big ).\] % \sigma \Big ( \sum_{i=1}^{n} w_ix_i + b \Big ) =
In this notion $\sigma$ is called the activation function, $\w$ the weights and $\b$ the bias of $\nu$.
\end{definition}

As a generalization of this concept, one can use multiple neurons in parallel to form a layer.

\begin{definition}  \label{def:layer}
Let $n_x, n_y \in \N$, $W = [\w_1, \dots, \w_{n_y}] \in \R^{n_x \times n_y}$, $\b \in \R^{n_y}$ and $\sigma : \R \to \R$. Given neurons $\nu_i$ with weights $\w_i$, bias $\b_i$ and activation function $\sigma$ for $i=1, \dots, n_y$, we define a layer as
\[ f : \R^{n_x} \to \R^{n_y} : x \mapsto \begin{bmatrix} \nu_1(x) \\ \vdots \\ \nu_{n_y}(x) \end{bmatrix} = \sigma \Big ( W\tr x + \b \Big ), \]
where the activation function $\sigma$ is applied element-wise.
\end{definition}

To provide a better understanding, these basic concepts can be visualized as graphs. \\

\begin{figure}[!h]
	\begin{minipage}[c]{0.5\linewidth}
	\centering
	\includegraphics{images/neuron.png}
      		\caption{Illustration of a neuron as graph.}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.5\linewidth}
	\centering
	\includegraphics{images/neuron.png}
      		\caption{Illustration of a layer as graph.}
	\end{minipage}
\end{figure}

In the following we will no longer distinguish between weights and biases and simply refer to them as parameters. Logically one can use the output of a layer as input for another layer. Iteratively doing so will construct specific functions known as neural networks. In order to formalize this idea we denote the following definition. \\

\pagebreak
\begin{definition} \label{def:network}
Let $l \in \N$ and $n_0, \dots, n_l \in \N$. A neural network is defined as a function
\[ f : \R^{n_0} \to \R^{n_l} : x \mapsto f_l \circ f_{l-1} \circ \cdots \circ f_1 (x),\]
where $f_i$ for $i=1, \dots, l$ is a layer with input dimension $n_{i-1}$ and output dimension $n_i$. Networks with a certain level of complexity, say $l>2$, are referred to as deep neural networks.
\end{definition}

Just like neurons and layers, neural networks can also be visualized as graphs.

\begin{figure}[!h]
\centering
\includegraphics{images/neuron.png}
\caption{Illustration of a neural network as graph.}
\label{fig:neuron}
\end{figure}  

Neural networks find massive adoption in machine learning as they can model arbitrary complex functions. For example one could envision functions that predict the weather at a specific place and time or functions that determine if an image contains a cat or a dog. Clearly there is no simple mathematical formula to describe such problems. \\

A common practice to learn those functions is, to fix the number of layers, the number of neurons per layer as well as the activation functions, to choose some initial parameters and then iteratively adjust the parameters in order to approximate the unknown target function. In the notion of definition \ref{def:network} one can determine the total number parameters in $f$ as
\[ n = \sum_{i=0}^{l-1} (n_i + 1) \cdot n_{i+1}. \]
Denoting these parameters as a vector $w \in \R^n$ enables us to define a function
\[ \tilde{f} : \R^{n_0} \times \R^n \to \R^{n_l} : (x,w) \mapsto \tilde{f}(x,w), \]
such that for fixed $w_1, w_2 \in \R^n$ the functions $\tilde{f}(x,w_1)$ and $\tilde{f}(x,w_2)$ are neural networks matching in their number of layers, number of neurons as well as activation functions and differing only in their choice of parameters. One calls $\tilde{f}(x,w_1)$ and $\tilde{f}(x,w_2)$ realizations of the neural network architecture $\tilde{f}$. \\

In the following, the terms neural network and neural network architecture will be used synonymously if the meaning is clear by the context. Unless otherwise specified, $n_0 \in \N$ will denote the dimension of the input space, $n_l \in \N$ will denote the dimension of the output space and $n \in \N$ will denote the number of adjustable parameters.

\subsection{Training in the Parameter Space}\label{sec:Training}

Training a neural network refers to finding the optimal parameters, given a fixed neural network architecture $\tilde{f} : \R^{n_0} \times \R^n \to \R^{n_l}$. In other words, finding a function $f \in \F|_{\tilde{f}}$, where
\[ \F|_{\tilde{f}} \coloneq \Big \{ f : \R^{n_0} \to \R^{n_l} : x \mapsto \tilde{f}(x,w) \mid w \in \R^n \Big \} \]
denotes the set of all neural networks with given architecture. In order to define a sense of optimality, we need a mechanism measuring the quality of network outputs. This is done by a loss function
\[ \ell: \R^{n_l} \times \R^{n_l} \to \R_+ : \big ( f(x,w),y \big ) \mapsto \ell \big ( f(x,w),y \big ) ,\]
that should be chosen in such a way, that for some input $x \in \R^{n_0}$ with target output $y \in \R^{n_l}$ it associates some cost with the error between the prediction $f(x,w)$ and the true label $y$. \\

Under the assumption, that in reality there exists a probability distribution $\D$ on $\R^{n_0} \times \R^{n_l}$, describing the correlation between inputs $x \in \R^{n_0}$ and target values $y \in \R^{n_l}$, we call a parameter vector $w_* \in \R^n$ to be optimal, if
\[ \forall w \in \R^n : R(w_*) \leq R(w), \]
where $R(w)$ denotes the so called risk or generalization error 
\[ R : \R^n \to \R_+ : w \mapsto \E_{(x,y) \sim \D } \Big [ \ell \big (f(x,w),y \big ) \Big ]. \]

For later use we will denote the marginal distribution of the inputs as $\D_x$. At this point it should be noted, that in general the underlying probability distribution $\D$ is unknown. Hence we can not evaluate the expectation in previous expression. \\

For this reason, the training of neural networks requires a set of $m \in \N$ labeled samples
\[ \Big \{ (x_i,y_i) \in \R^{n_0} \times \R^{n_l} \mid i=1, \dots, m \Big \}, \]
which is called training data. Instead of working with the generalization error, the training data enables us to minimize the empirical error function
 \[ E : \R^n \to \R_+ : w \mapsto \frac{1}{m} \sum_{i=1}^{m} \ell \big ( f(x_i,w),y_i \big) + \lambda \big \| w \big \|_2^2, \]
where $\lambda \geq 0$ is some regularization parameter used to control the complexity of $w$. \\
 
Intuitively, minimization of the empirical error should lead to a minimization of the generalization error as well. Currently, a lot of research is being done on how these measures relate to each other. Nevertheless this thesis will focus on how to minimize the empirical error given labeled training data. \\

The problem of finding some parameter vector $w$ that minimizes the empirical error function is usually hard to solve as the error may have highly nonlinear dependencies on the parameters. This is why in practice one often aims to compare several local minima and to settle for one that is sufficient enough, regardless of whether it is a global minimum or not.

\begin{definition}
Let $f: \R^n \to \R$ be differentiable. The gradient of $f$ is defined as
\[ \nabla f : \R^n \to \R^n : x = (x_1, \dots, x_n) \mapsto \bigg [ \frac{\partial}{\partial x_1} f(x), \cdots, \frac{\partial}{\partial x_n} f(x) \bigg ]\tr . \]
\end{definition}

Due to the complexity of the error function, in general there is no easy way to find an analytical solution to the problem $\nabla E(w) = 0$. Hence most of the techniques for error minimization are based on iterative approximation. One popular method is gradient descent, which was first raised by Cauchy in \cite{GD}.

\begin{algorithm}
\caption{Gradient Descent} \ \\
\textcolor{white}{$\Big |$}1: Requires differentiable $f: \R^n \to \R$, random $x_0 \in \R^n$ and $\epsilon \geq 0$. \\
\textcolor{white}{$\Big |$}2: Initialize $k \leftarrow 0$. \\
\textcolor{white}{$\Big |$}3: \textbf{while} $ \big \| \nabla f(x_k) \big \|_2 > \epsilon $ \textbf{do}:\\
\textcolor{white}{$\Big |$}4: \quad Choose a step size $\alpha_k > 0$. \\
\textcolor{white}{$\Big |$}5: \quad Set $x_{k+1} \leftarrow x_k - \alpha_k \cdot \nabla f(x_k)$ and $k \leftarrow k+1$. \\
\textcolor{white}{$\Big |$}6: \textbf{end while}
\end {algorithm}

We will see, that in the case where $f$ is bounded from below and $\nabla f$ is Lipschitz continuous, we can guarantee the  convergence of gradient descent to a stationary point.

\begin{definition}
Let $(\X,d_{\X})$ and $(\Y, d_{\Y})$ be two metric spaces. A function $f: \X \to \Y$ is called Lipschitz continuous, if there exists $L \geq 0$, such that
\[ \forall x,x' \in \X : d_{\Y} \big ( f(x) , f(x') \big ) \leq L \cdot d_{\X}(x,x'). \]
The smallest constant $L$, that suffices the previous condition is called Lipschitz constant of $f$.
\end{definition}

Casually speaking, the Lipschitz continuity of $\nabla f$ ensures that the gradient does not change heavily for two points that are close to each other.

\begin{definition}
Let $f: \R^n \to \R$ be differentiable. A stationary point of $f$ is an element of
\[ \big \{ x \in \R^n \mid \nabla f(x) = 0 \big \}. \]
\end{definition}

There a three kinds of stationary points: saddle points, local extrema and global extrema. We are especially interested in global minima as they provide the lowest possible error.

\begin{definition}
Let $\X$ be a subset of some real vector space and $f: \X \to \R$ be bounded from below. We call $x_* \in \X$ a global minimizer of $f$, if
\[ \forall x \in \X : f(x_*) \leq f(x). \]
\end{definition}

To prove convergence of the gradient descent algorithm for bounded $f$ with Lipschitz continuous gradient, we need the following result which is formulated as lemma 1.2.3 in \cite{ConvexOptimization}.

\begin{lemma} \label{lem:descent}
Let $f: \R^n \to \R$ be differentiable, such that the gradient $\nabla f$ is Lipschitz continuous with Lipschitz constant $L \geq 0$, then
\[ \forall x,x' \in \R^n : f(x') \leq f(x) + \big \langle \nabla f(x) , x' -x \big \rangle + \frac{L}{2} \big \| x'- x \big \|_2^2. \]
\end{lemma}

\begin{proof}
Let $x,x' \in \R^n$, define $z_{\lambda} \coloneq x + \lambda (x' - x)$ for $\lambda \in \R$ and consider the function
\[ \psi : \R^n \to \R : \lambda \mapsto f(z_{\lambda}). \]
Since $f$ is differentiable we can apply the chain rule to derive
\[ \psi' : \R^n \to \R : \lambda \mapsto (x'-x)\tr \cdot \nabla f(z_\lambda) . \]
Integration over $\psi'$ from $\lambda = 0$ to $\lambda = 1$ yields
\[ \int_{0}^{1} \big \langle \nabla f(z_{\lambda}), x'-x \big \rangle \ d\lambda = \int_{0}^{1} \psi'(\lambda) \ d \lambda = \psi(1) - \psi(0) = f(x') - f(x). \]
This equality can be rewritten as
\[ \begin{split} 
f(x') - f(x) 
&= \int_{0}^{1}\big \langle \nabla f(z_{\lambda}), x'-x \big \rangle \ d\lambda \\\
&= \int_{0}^{1} \big \langle \nabla f(z_{\lambda}) - \nabla f(x) + \nabla f(x) , x'-x\big \rangle \ d\lambda \\\
&= \big \langle \nabla f(x), x'-x \big \rangle + \int_{0}^{1} \big \langle \nabla f(z_{\lambda}) - \nabla f(x), x'-x \big \rangle \ d\lambda.
\end{split} \]
Using the Cauchy-Schwarz inequality we derive
\[ \begin{split}
\Big | \ f(x') - f(x) - \big \langle \nabla f(x), x'-x \big \rangle \ \Big | 
&= \Big | \ \int_{0}^{1} \big \langle \nabla f(z_{\lambda}) - \nabla f(x), x'-x \big \rangle \ d\lambda \ \Big | \\\
&\leq \int_{0}^{1} \Big | \ \big \langle \nabla f(z_{\lambda}) - \nabla f(x), x'-x \big \rangle \ \Big | \ d\lambda \\\
&\leq \int_{0}^{1} \big \| \nabla f( z_{\lambda}) - \nabla f(x) \big \|_2 \cdot \big \| x'-x \big \|_2 \ d\lambda.
\end{split} \]
Due to the Lipschitz continuity of $\nabla f$ we can use the upper bound 
\[ \big \| \nabla f( z_{\lambda}) - \nabla f(x) \big \|_2 \leq L \cdot \big \| z_{\lambda} - x \big \|_2 = L \cdot \big \| \lambda(x'-x) \big \|_2 = L\lambda \cdot \big \| x'-x \big \|_2 \]
for $\lambda \in [0,1]$, to conclude
\[ \Big | \ f(x') - f(x) - \big \langle \nabla f(x), x'-x \big \rangle \ \Big | \leq \int_{0}^{1} L\lambda \cdot \big \| x'-x \big \|_2^2 \ d\lambda = \frac{L}{2} \big \| x'-x \big \|_2^2. \]
Rearranging the inequality finally yields
\[ f(x') \leq f(x) + \big \langle \nabla f(x) , x' -x \big \rangle + \frac{L}{2} \big \| x' - x \big \|_2^2. \]
This completes the proof, since $x,x'$ were chosen arbitrary.
\end{proof}

\begin{theorem} \label{thm:descent}
Let $f: \R^n \to \R$ be differentiable with a global minimizer $x_* \in \R^n$, such that the gradient $\nabla f$ is Lipschitz continuous with Lipschitz constant $L>0$, then gradient descent with $\epsilon = 0$ and constant step size $\alpha_k = 1/L$ produces a sequence $(x_k)_{k=0}^\infty$ in $\R^n$, such that 
\[ \forall m \in \N : \min_{0 \leq k \leq m} \big \| \nabla f(x_k) \big \|_2^2 \leq \frac{2L \big ( f(x_0) - f(x_*) \big )}{m+1}. \]
\end{theorem}

\begin{proof}
Let $k \in \N$ be arbitrary. Since $f$ is differentiable with Lipschitz continuous gradient, we can apply lemma \ref{lem:descent} to derive
\[  f(x_{k+1}) \leq f(x_k) + \big \langle \nabla f(x_k) , x_{k+1} -x_k \big \rangle + \frac{L}{2} \big \| x_{k+1} - x_k \big \|_2^2. \]
By the definition of gradient descent, we can plug in $x_{k+1} - x_k = - 1/L \cdot \nabla f(x_k)$ to conclude
\[ \begin{split} 
f(x_{k+1}) 
&\leq f(x_k) + \big \langle \nabla f(x_k) , - \frac{1}{L} \nabla f(x_k) \big \rangle + \frac{L}{2} \big \| \frac{1}{L} \nabla f(x_k) \big \|_2^2 \\\
&= f(x_k) - \frac{1}{L} \big \| \nabla f(x_k) \big \|_2^2 + \frac{1}{2L} \big \| \nabla f(x_k) \big \|_2^2 \\\
&= f(x_k) - \frac{1}{2L} \big \| \nabla f(x_k) \big \|_2^2.
\end{split} \]
Hence the gradient descent algorithm with constant step size $\alpha_k = 1/L$ guarantees to make progress unless $\nabla f(x_k) \neq 0$. The previous expression is equivalent to
\[ \big \| \nabla f(x_k) \big \|_2^2 \leq 2L \big ( f(x_k) - f(x_{k+1}) \big ). \]
Summing up both sides from $k=0$ to some $m \in \N$ and taking the average results in
\[ \frac{1}{m+1} \sum_{k=0}^{m} \big \| \nabla f(x_k) \big \|_2^2 \leq \frac{2L}{m+1} \sum_{k=0}^{m} f(x_k) - f(x_{k+1}) = \frac{2L}{m+1} \big ( f(x_0) - f(x_{m+1}) \big ). \]
Using $f(x_*) \leq f(x_k)$ for every $k \in \N_0$, we conclude
\[ \min_{0 \leq k \leq m} \big \| \nabla f(x_k) \big \|_2^2 \leq \frac{1}{m+1} \sum_{k=0}^{m} \big \| \nabla f(x_k) \big \|_2^2 \leq \frac{2L}{m+1} \big ( f(x_0) - f(x_*) \big ). \]
This completes the proof, since $m$ was chosen arbitrary.
\end{proof}

The theorem can be found as equation (1.2.15) in the beginning of section 1.2.3 of \cite{ConvexOptimization}. Note that in practice it is inefficient to use a constant step size of $1/L$. Nevertheless it is useful for theoretical analysis to guarantee convergence. \\

Theorem \ref{thm:descent} states, that in the limit as $n \to \infty$, gradient descent converges to a stationary point $x \in \R^n$ with $\nabla f(x) = 0$. Since we have seen in the proof, that $f(x_k)$ is monotonically decreasing in $k$, gradient descent converges to the next stationary point in direction of descent, which is either a local minimum, a global minimum or a saddle point. \\

Next we will define a special class of functions having the nice property, that every stationary point is a global minimum and therefore gradient descent is an excellent tool for minimization.

\begin{definition}
Let $\X$ be a subset of some real vector space. $\X$ is said to be convex, if 
\[ \forall x, x' \in \X : \forall \lambda \in [0,1] : x + \lambda (x' - x) \in \X. \]
\end{definition}

Casually speaking a set is convex if it contains the connection line between any two points in the set. Based on the notion of convex sets we can define convex functions.

\begin{definition}
Let $\X$ be a convex subset of some real vector space. A function $f: \X \to \R$ is said to be convex, if
\[ \forall x,x' \in \X : \forall \lambda \in [0,1] : f \big (x + \lambda (x'-x) \big ) \leq f(x) + \lambda \big ( f(x') - f(x) \big ). \]
\end{definition}

For later use we denote the following property of convex functions.

\begin{lemma} \label{lem:convexity}
Let $\X$ be a convex subset of some real vector space. For convex functions $f: \X \to \R$ and $g: \X \to \R$ the sum $f+g$ is convex as well.
\end{lemma}

\begin{proof}
Let $x,x' \in \X$ and $\lambda \in [0,1]$ be arbitrary, then
\[ \begin{split}
\big ( f+g \big ) \big (x + \lambda (x'-x) \big ) 
&= f \big ( x + \lambda (x'-x) \big ) + g \big (x + \lambda (x'-x) \big ) \\\
\textcolor{white}{\Big |} &\leq f(x) + \lambda \big ( f(x') - f(x) \big ) + g(x) + \lambda \big ( g(x') - g(x) \big ) \\\
&= ( f + g )(x) + \lambda \big ( (f + g)(x') - (f+g)(x) \big ).
\end{split} \]
This shows the convexity of $f+g$.
\end{proof}

Functions that are convex and continuous differentiable have the nice property, that every stationary point is a global minimum. Mathematically this can be formulated as follows.

\begin{lemma}
Let $f: \R^n \to \R$ be convex and continuous differentiable and $x \in \R^n$, then
\[ \nabla f(x) = 0 \ \Leftrightarrow \ \forall x' \in \R^n : f(x) \leq f(x').  \]
\end{lemma}

\begin{proof}
Let $x \in \R^n$ with $\nabla f(x) = 0$. For arbitrary $x' \in \R^n$ we define the function
\[ \psi : \R \to \R : \lambda \mapsto f(x) + \lambda \big ( f(x') - f(x) \big ) - f \big ( x + \lambda (x'-x) \big ), \]
which is non-negative on the interval $[0,1]$ by convexity of $f$. Denoting the derivative as
\[ \psi' : \R \to \R : \lambda \mapsto f(x') - f(x) - (x'-x) \tr \cdot \nabla f \big (x + \lambda(x'-x)\big ) \]
and using the non-negativity of $\psi$ together with $\psi(0) = 0$, we observe that
\[ \psi'(0) = f(x') - f(x) - (x'-x) \tr \cdot \nabla f(x) \geq 0. \]
Since $x$ was chosen such that $\nabla f(x) = 0$, we can rearrange terms to conclude
\[ \forall x' \in \R^n : f(x) \leq f(x'). \]
This completes the proof, since the backwards direction is trivial.
\end{proof}

Thus for lower bounded functions $f: \R^n \to \R$, that are convex and differentiable with Lipschitz continuous gradient $\nabla f$, we can guarantee the convergence of gradient descent to a global minimum due to theorem \ref{thm:descent}. Unfortunately this does not apply to the empirical error function used for training neural networks, but we will introduce a theoretical approach to prevent this problem in section \ref{sec:kernel}. Next we will provide further iterative optimization methods. \\

The gradient descent method is based on the first order Taylor expansion. Intuitively, inclusion of second order information should speed up the minimization process. This idea lead to the famous Newton's method, which requires the objective function to be twice differentiable. Furthermore, to apply Newton's method, we have to compute the Hessian matrix, leading to way more computation effort needed in comparison to gradient descent. In order to reduce this additional effort, one can approximate the inverse Hessian matrix instead of computing it exactly. One popular method, that pursues this approach is the following from \cite{BFGS}.

\begin{algorithm}
\caption{Broyden-Fletcher-Goldfarb-Shanno (BFGS)} \ \\
\textcolor{white}{$\Big |$}1: Requires twice-differentiable $f: \R^n \to \R$, random $x_0 \in \R^n$ and  $\epsilon \geq 0$. \\
\textcolor{white}{$\Big |$}2: Initialize $k \leftarrow 0$, $B_0 \leftarrow I_n$. \\
\textcolor{white}{$\Big |$}3: \textbf{while} $ \big \| \nabla f(x_k) \big \|_2 > \epsilon $ \textbf{do}: \\
\textcolor{white}{$\Big |$}4: \quad Choose a step size $\alpha_k > 0$. \\
\textcolor{white}{$\Big |$}5: \quad Set $x_{k+1} \leftarrow x_k - \alpha_k B_k \nabla f(x_{k})$. \\
\textcolor{white}{$\Big |$}6: \quad Let $s_k = x_{k+1} - x_{k}$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_{k})$. \\
\textcolor{white}{$\Big |$}7: \quad Let $\rho_k = (y_k\tr s_k)^{-1}$ and $V_k = I_n - \rho_k y_k s_k\tr $. \\
\textcolor{white}{$\Big |$}8: \quad Set $B_{k+1} \leftarrow V_k\tr B_{k}V_k + \rho_k s_k s_k\tr $ and $k \leftarrow k+1$. \\
\textcolor{white}{$\Big |$}9: \textbf{end while}
\end{algorithm}

With the initialization $B_0 = I_n$, the method starts with a simple gradient descent update and then iteratively minimizes the objective function by inverse Hessian matrix approximations. \\

Unfortunately, Hessian matrix computation or Hessian matrix approximation is infeasible for deep neural networks due to the large number of parameters. However, later on we will introduce an approach to train neural networks in tiny subspaces, such that second order methods become applicable. \\

At this stage we are restricted to training neural networks via gradient descent on the empirical error function. Recalling its definition
 \[ E : \R^n \to \R_+ : w \mapsto \frac{1}{m} \sum_{i=1}^{m} \ell \big ( f(x_i,w),y_i \big) + \lambda \big \| w \big \|_2^2, \]
we observe, that the computation effort needed to compute the gradient, depends on the number of training samples $m \in \N$. This is a severe problem for the gradient descent algorithm, since we have to train on a significant amount of data, to optimize the millions of parameters in deep neural networks. Thus, gradient descent on the empirical error function is computationally and time expensive, that even with heavy computing power it can be unfeasible to find good solutions in a reasonable period of time. However, there exist several other optimization methods, that apply the same concept of minimizing a function along the slope of its surface. These algorithms make a trade-off between the accuracy and the time needed for gradient computations. In the following we will introduce two of them. \\

Stochastic gradient descent is a variation of gradient descent, that aims to approximate the gradient of the objective function instead of computing it exactly. The basic idea is to compute the gradient on a random fraction of the data set and use this as an estimation of the gradient on the whole data set. In each iteration $k \in \N$ one randomly draws a subset $\I_k \subset \{ 1, \dots, m \}$ of size $| \I_k | = b \in \N$ and uses
\[ \nabla E(w_k,\I_k) \coloneq \frac{1}{b} \sum_{i \in \I_k}^{}  \ell \big ( f(x_i,w_k),y_i \big) + \lambda \big \| w_k \big \|_2^2 \]
as an approximation of $\nabla E(w_k)$. Although only a small amount of data is used for one single parameter update, during training most of the data will be used to fit the model due to the large number of iterative updates. Formally stochastic gradient descent proceeds as follows.

\begin{algorithm}
\caption{Stochastic Gradient Descent (SGD)} \ \\
\textcolor{white}{$\Big |$}1: Requires differentiable empirical error function $E: \R^n \to \R$ and random $w_0 \in \R^n$. \\
\textcolor{white}{$\Big |$}2: Requires a batch size $b \in \N$ and a number of epochs $K \in \N$. \\
\textcolor{white}{$\Big |$}3: \textbf{for} $k=1, \dots, \frac{Km}{b}$ \textbf{do}: \\
\textcolor{white}{$\Big |$}4: \quad Draw a random subset $\I_k \subset \{1, \dots, m \}$ of size $| \I_k | = b$. \\
\textcolor{white}{$\Big |$}5: \quad Choose a step size $\alpha_k > 0$. \\
\textcolor{white}{$\Big |$}6: \quad Set $w_{k+1} \leftarrow w_k - \alpha_k \cdot \nabla E(w_k,\I_k)$. \\
\textcolor{white}{$\Big |$}7: \textbf{end for}
\end{algorithm}

In here, an epoch describes the use of a number of samples for parameter updates, that equals the size of the whole training dataset. Thus on average each individual training sample will contribute $K$ times to the optimization process. In comparison to this, the classical gradient descent method uses the data of one epoch per iteration. A theoretical convergence analysis of stochastic gradient descent can be found in \cite{SGD}, but would exceed the scope of this thesis. \\

So far we have not discussed how to choose the step sizes $\alpha_k$. To simplify things, in practice one usually chooses a step size $\alpha > 0$, that stays constant across each iteration. Unfortunately a constant step size does not adapt to the specific local properties of the error surface at iteration $k \in \N$. One method that aims to improve the stochastic gradient descent algorithm by including past gradients for the current parameter update is called adaptive moment estimation and was first raised in \cite{Adam}. \\

\begin{algorithm}
\caption{Adaptive Moment Estimation (Adam)} \ \\
\textcolor{white}{$\Big |$}1: Requires differentiable empirical error function $E: \R^n \to \R$ and random $w_0 \in \R^n$.  \\
\textcolor{white}{$\Big |$}2: Requires constant step size $\alpha > 0 \in \R$, batch size $b \in \N$ and number of epochs $K \in \N$. \\
\textcolor{white}{$\Big |$}3: Requires momentum factors $\beta_1, \beta_2 \in [0,1)$ and a scalar $\epsilon > 0$. \\
\textcolor{white}{$\Big |$}4: \textbf{for} $k=1, \dots, \frac{Km}{b}$ \textbf{do}: \\
\textcolor{white}{$\Big |$}5: \quad Draw a random subset $\I_k \subset \{1, \dots, m \}$ of size $| \I_k | = b$. \\
\textcolor{white}{$\Big |$}6: \quad Set $m_{k} \leftarrow \beta_1 m_{k-1} + (1-\beta_1) \cdot \nabla E(w_k,\I_k)$ and let $\hat{m}_k \coloneq \frac{m_k}{1-\beta_1^k}$. \\
\textcolor{white}{$\Big |$}7: \quad Set $v_{k} \leftarrow \beta_2 v_{k-1} + (1-\beta_2) \cdot \nabla E(w_k,\I_k)^2$ and let $\hat{v}_k \coloneq \frac{v_k}{1-\beta_2^k}$. \\
\textcolor{white}{$\Big |$}8: \quad Set $w_{k+1} \leftarrow w_k - \alpha \cdot \frac{\hat{m}_k}{\sqrt{\hat{v}_k} + \epsilon}$. \\
\textcolor{white}{$\Big |$}9: \textbf{end for}
\end{algorithm}

In here, all operations on vectors are performed element-wise. The scalar $\epsilon$ is used to prevent division by zero. The default setup proposed by the authors is to choose $\epsilon = 10^{-8}$, $\beta_1 = 0.9$ and $\beta_2 = 0.999$. We will always apply this default setting when working with Adam. \\

This completes the necessary basics on notation and training of neural networks. Next we will investigate certain approaches on how to improve the training of neural networks in order prevent the curse of non-convexity and reduce the computation and time effort needed.  \\

\textbf{Exploration:} Further second order method \\
\textbf{Optional:} Backpropagation

\pagebreak
\section{Training in the Function Space} \label{sec:kernel}

The goal of this section is to introduce and investigate the neural tangent kernel, which was analyzed by Arthur Jacot, Franck Gabriel and Cl\'{e}ment Holger in \cite{NTK}. We start by motivating and giving the definition of functional derivatives and functional gradient descent. This leads to the definition of multi-dimensional kernels, which enable us to generalize the functional gradient descent to so called kernel gradient descent. Having this concept in mind, we will prove that the training of neural networks in the parameter space is linked to kernel gradient descent in the function space with respect to a special kernel, the neural tangent kernel.

\subsection{Functional Gradient Descent}

As seen in section \ref{sec:Training}, the training of neural networks via parameter optimization is usually accomplished by minimizing the empirical error
\[ E : \R^n \to \R_+ : w \mapsto \frac{1}{m} \sum_{i=1}^{m} \ell \big ( f(x_i,w),y_i \big) + \lambda \big \| w \big \|_2^2 \]
on labeled training data $\{ (x_i,y_i) \}_{i=1}^m \subset \R^{n_0} \times \R^{n_l}$. Due to the non-convexity of the error function $E$, it is in general hard to find a global minimum. However, for theoretical analysis, this problem can be prevented by moving from the parameter space $\R^n$ to the function space
\[ \F \coloneq \Big \{ f: \R^{n_0} \to \R^{n_l} \Big \}. \]
That is we are no longer optimizing the specific parameters of the unknown target function but rather the function itself. This approach requires the definition of a cost functional
\[ C: \F \to \R_+ : f \mapsto \frac{1}{m} \sum_{i=1}^{m} \ell \big ( f(x_i),y_i \big), \]
associating some cost to each element of the function space $\F$. The interpretation of $\ell$ as a convex function in $f$ provides a convex cost $C$, since by lemma \ref{lem:convexity} the sum of convex functions is convex again. This is a huge advantage in comparison to the parameter optimization approach, where the convexity does not translates to $E$ due to the composition of $\ell$ with $f$. \\

We will introduce the functional gradient descent method, that enables us to minimize $C$ in the function space $\F$ and is an analog to the gradient descent method in the parameter space $\R^n$. Since $C$ is convex in the function space, convergence to a global minimum will be guaranteed in contrast to the non-convex parameter optimization problem. \\

Note that this approach is great for theoretical analysis due to the advantage of convexity. In practice however, we are still in need of the explicit parameters $w \in \R^n$, that describe the objective function. Nevertheless we can use the analysis of functional gradient descent to get a better understanding of gradient descent in the parameter space. \\

To start things off, we need the definition of functionals, mapping each element of the function space $\F$ to a real value. The cost $C$ is an example of such a functional.

\begin{definition}
A functional on the vector space $\F$ is defined as a mapping
\[ \mu: \F \to \R : f \mapsto \mu[f]. \]
\end{definition}

To minimize functionals via the same approach in gradient descent, we need to translate the well known concepts of differentiability and gradients from the parameter space $\R^n$ to the function space $\F$. The following definition can be found in Appendix A of \cite{Functionals}.

\begin{definition} \label{def:derivative}
Let $\phi, f_0 \in \F$. A functional $\mu: \F \to \R$ is said to be differentiable at the point $f_0$ in direction $\phi$, if
\[ \int \frac{\partial\mu}{\partial f_0(x)}(x)\phi(x) \ dx = \lim_{\epsilon \to 0} \frac{\mu[f_0 + \epsilon \phi] - \mu[f_0]}{\epsilon} \]
exists. The functional gradient of $\mu$ at $f_0$ is defined as 
\[ \nabla \mu |_{f_0} : \R^{n_0} \to \R^{n_l} : x \mapsto \frac{\partial \mu}{\partial f_0(x)}(x). \]
 If the limit exists for every $\phi \in \F$ we define the functional derivative of $\mu$ at $f_0$ as
\[ \partial_{f} \mu |_{f_0} : \F \to \R : \phi \mapsto \frac{\partial}{\partial \epsilon} \Big [ \mu[f_0+\epsilon \phi] \Big ]_{\epsilon=0}. \]
\end{definition}

Based on these definitions, we can iteratively minimize functionals in a similar manner to the gradient descent method. Formally this is achieved as follows.

\begin{algorithm}
\caption{Functional Gradient Descent} \ \\
\textcolor{white}{$\Big |$}1: Requires differentiable $\mu: \F \to \R$, random $f_0 \in \F$ and $\epsilon \geq 0$. \\
\textcolor{white}{$\Big |$}2: Initialize $k \leftarrow 0$. \\
\textcolor{white}{$\Big |$}3: \textbf{while} $\big \| \nabla \mu |_{f_k} \big \|_{\D_x} > \epsilon$ \textbf{do}: \\
\textcolor{white}{$\Big |$}4: \quad Choose a step size $\alpha_k > 0$. \\
\textcolor{white}{$\Big |$}5: \quad Set $f_{k+1} \leftarrow f_k - \alpha_k \cdot \nabla \mu |_{f_k}$ and $k \leftarrow k+1$. \\
\textcolor{white}{$\Big |$}6: \textbf{end while}
\end {algorithm}

With a slight change in notation, the same argumentation that was used for convergence of gradient descent on functions, can be applied to the functional gradient descent algorithm. \\

Thus, the choice of a convex loss $\ell$, which yields a convex cost functional $C$, enables us to guarantee convergence of functional gradient descent to a global minimum. As said before, this is great for theoretical considerations. In practice however, we are still in need of the explicit parameters $w \in \R^n$, that describe the objective function. \\

To still derive conclusions for the parameter training based on the properties of training in the function space, we need to establish a relationship between the two minimization approaches. This can be achieved by the use of kernels. Before we explain this idea in detail, we need some background knowledge from functional analysis. \\

\textbf{To Do:} A la "In general, you can parametrize any function in a number of ways, each parametrization gives rise to different steps in gradient descent." \\
\textbf{Optional:} Proofs for convergence of Functional Gradient Descent. 

\pagebreak
\subsection{The Dual Space}

In the following the focus is on functionals that are linear and continuous. We will see, that these kind of functionals can be represented by elements of the function space $\F$, which is a crucial ingredient for the further theory. More precisely, we will construct a surjective mapping from $\F$ to the set of linear and continuous functionals on $\F$, the dual space.

\begin{definition}
Given a real vector space $\F$, we define the dual space
\[ \F^* \coloneq \Big \{ \mu : \F \to \R \mid \mu \text{ linear and continuous} \Big \}. \]
\end{definition}

At this point it is unclear how all of the elements in $\F^*$ arise. As mentioned, they can be constructed by elements of the function space $\F$, for which we need to equip  $\F$ with a semi-norm. This procedure is based on the following result from \cite{NTK}.

\begin{lemma}
Given a fixed probability distribution $\D_x$ on the input space $\R^{n_0}$, one can equip the function space $\F$ with the seminorm
\[ \| \cdot \|_{\D_x} : \F \to \R : f \mapsto \sqrt{\langle f , f \rangle_{\D_x}} \] 
in terms of the symmetric positive semidefinite bilinear form
\[ \langle \cdot,\cdot \rangle_{\D_x} : \F \times \F \to \R : (f,g) \mapsto \langle f,g \rangle_{\D_x} \coloneq \E_{x \sim \D_x} \Big [ f(x)\tr g(x) \Big ].\]
\end{lemma}

\begin{proof}
Let $f_1, f_2, f, g \in \F$ and $\lambda \in \R$. $\langle \cdot,\cdot \rangle_{\D_x}$ is indeed symmetric and bilinear since it suffices \\

(i) $\langle f_1 + f_2 , g \rangle_{\D_x} = \langle f_1 , g \rangle_{\D_x} + \langle f_2 , g \rangle_{\D_x}$:
\[ \begin{split}
\langle f_1 + f_2 , g \rangle_{\D_x}
&= \E_{x \sim \D_x} \Big [ \big ( f_1(x) + f_2(x) \big )\tr g(x) \Big ] \\\
&= \E_{x \sim \D_x} \Big [ f_1(x)\tr g(x) + f_2(x)\tr g(x) \Big ] \\\
&= \E_{x \sim \D_x} \Big [ f_1(x)\tr g(x) \Big ] + \E_{x \sim \D_x} \Big [ f_2(x)\tr g(x) \Big ] \\\
&= \langle f_1 , g \rangle_{\D_x} + \langle f_2 , g \rangle_{\D_x},
\end{split} \]

(ii) $\langle \lambda f , g \rangle_{\D_x} = \lambda \langle f, g \rangle_{\D_x}$:
\[ \langle \lambda f , g \rangle_{\D_x} = \E_{x \sim \D_x} \Big [ \lambda f(x)\tr g(x) \Big ] = \lambda \cdot \E_{x \sim \D_x} \Big [ f(x)\tr g(x) \Big ] = \lambda \langle f, g \rangle_{\D_x}, \]

(iii) $\langle f , g \rangle_{\D_x} = \langle g , f \rangle_{\D_x}$:
\[ \langle f , g \rangle_{\D_x} = \E_{x \sim \D_x} \Big [ f(x)\tr g(x) \Big ] = \E_{x \sim \D_x} \Big [ g(x)\tr f(x) \Big ] = \langle g , f \rangle_{\D_x}. \]

Furthermore $\langle \cdot,\cdot \rangle_{\D_x}$ is positive semidefinite, since for any $f \in \F$ it holds, that
\[ \langle f , f \rangle_{\D_x} =  \E_{x \sim \D_x} \Big [ f(x)\tr f(x) \Big ] = \E_{x \sim \D_x} \Big [ \sum_{i=1}^{n_l} f_i(x)^2 \Big ] \geq 0. \]

Thus $\| \cdot \|_{\D_x} = \sqrt{\langle \cdot , \cdot \rangle_{\D_x}}$ is a seminorm on the function space $\F$.
\end{proof}

Just as norms induce metrics, semi-norms induce pseudometrics, so that we can define a distance between two elements of $\F$ in terms of the bilinear form $\langle \cdot, \cdot \rangle_{\D_x}$.

\begin{lemma}
The seminorm $\| \cdot \|_{\D_x}$ induces the pseudometric
\[ d_{\F}: \F \times \F \to \R_+ : (f,g) \mapsto \big \| f-g \big \|_{\D_x}, \]
such that $(\F,d_{\F})$ is a complete pseudometric space.
\end{lemma}

\begin{proof}
Let $f,g,h \in \F$. $d_{\F}$ is indeed a pseudometric since it suffices \\

(i) $d_{\F}(f,f) = 0$:
\[ d_{\F}(f,f) = \big \| f - f \big \|_{\D_x} = 0, \]

(ii) $d_{\F}(f,g) = d_{\F}(g,f)$:
\[ d_{\F}(f,g) = \big \| f - g \big \|_{\D_x} = \big \| g - f \big \|_{\D_x} = d_{\F}(g,f), \]

(iii) $d_{\F}(f,h) \le d_{\F}(f,g) + d_{\F}(g,h)$:
\[ \begin{split} 
d_{\F}(f,h) &= \big \| f - h \big \|_{\D_x} \\\ &= \big \| f - g + g - h \big \|_{\D_x} \\\ &\le \big \| f - g  \big \|_{\D_x} + \big \| g - h \big \|_{\D_x} = d_{\F}(f,g) + d_{\F}(g,h). 
\end{split} \]

The upper equations follow directly from the properties of the semi-norm $\| \cdot \|_{\D_x}$. \\

It remains to show the completeness of $\F$, which arises from the fact, that every Cauchy sequence in $\R$ converges. To clarify this, we let $(f_n)_{n=1}^\infty$ denote a Cauchy sequence in $\F$, so
\begin{equation} \tag{$\ast$} \forall \epsilon > 0 : \exists N \in \N : \forall m,n > N : d_{\F}(f_n, f_m)^2 < \epsilon. \end{equation}
Recalling the definition of $\| \cdot \|_{\D_x}$, we have
\[ d_{\F}(f_n ,f_m)^2 = \big \| f_n - f_m \big \|_{\D_x}^2 = \E_{x \sim \D_x} \Big [ ( f_n - f_m )(x) \tr ( f_n - f_m )(x) \Big ]. \]
Using this and the notation $(\hat{f}_n)_i \coloneq \E_{x \sim \D_x} \big [ ( f_n)_i(x) \big ]$, the inequality in $(\ast)$ translates to
\[ d_{\F}(f_n ,f_m)^2 = \E_{x \sim \D_x} \Big [ \sum_{i=1}^{n_l} (f_n - f_m)_i(x)^2 \Big ] = \Big [ \sum_{i=1}^{n_l} \big ( ( \hat{f}_n)_i - (\hat{f}_m)_i \big )^2 \Big ] < \epsilon. \]
Since all of the summands are non-negative, we observe for $i=1,\dots, n_l$, that
\[ \forall \epsilon > 0 : \exists N \in \N : \forall m,n > N : \big | (\hat{f}_n)_i - (\hat{f}_m)_i \big | < \epsilon. \]
Hence $((\hat{f}_n)_i)_{n=1}^\infty$ is a Cauchy sequence in $\R$ and therefore converges to a limit
\[ \hat{f}_i \coloneq \lim_{n \to \infty} (\hat{f}_n)_i \in \R. \]
Equivalently this can be formulated as
\[ \forall \epsilon > 0 : \exists N \in \N : \forall n > N : \big | (\hat{f}_n)_i - \hat{f}_i \big | < \epsilon. \]
Using these limits we can construct a function $f_\textit{lim} \in \F$ as
\[ f_\textit{lim}: \R^{n_0} \to \R^{n_l} : x \mapsto \Big [ \hat{f}_1, \dots, \hat{f}_{n_l} \Big ] \tr. \]
Again, using $(\hat{f}_n)_i = \E_{x \sim \D_x} \big [ ( f_n)_i(x) \big ]$ and the definition of $\| \cdot \|_{\D_x}$, we observe for $n \in \N$, that
\[ d_{\F}(f_n ,f_\textit{lim})^2 = \E_{x \sim \D_x} \Big [ \sum_{i=1}^{n_l} (f_n - f_\textit{lim})_i(x)^2 \Big ] = \Big [ \sum_{i=1}^{n_l} \big ( ( \hat{f}_n)_i - \hat{f}_i \big )^2 \Big ]. \]
Since each sequence $((\hat{f}_n)_i)_{n=1}^\infty$ converges, we finally conclude, that
\[ \forall \epsilon > 0 : \exists N \in \N : \forall n > N : d_{\F}(f_n, f_\textit{lim})^2 < \epsilon. \]
Hence the Cauchy sequence $(f_n)_{n=1}^\infty$ has a limit in $\F$, which makes $(\F,d_{\F})$ complete.
\end{proof}

Note that since $(\F, d_{\F})$ is only a pseudometric space and not a metric space, the limit $f_\textit{lim}$ constructed in the proof is not unique. Essentially any $f \in \F$, that suffices 
\[ \E_{x \sim \D_x} \Big [ f(x) \Big ] = \Big [ \hat{f}_1, \dots, \hat{f}_{n_l} \Big ] \tr, \]
represents a limit function of the Cauchy sequence. This is due to the limit being measured in terms of the semi-norm $\| \cdot \|_{\D_x}$. \\

Based on the completeness of $(\F, d_\F)$, we can introduce the concept of orthogonal projections, which is necessary for our goal of establishing a mapping between $\F$ and its dual space $\F^*$. The following lemma is a translation of Theorem 3.3.1 in \cite{FunctionalAnalysis} to our case, where we have a symmetric positive semidefinite bilinear form $\langle \cdot, \cdot \rangle_{\D_x}$ instead of an inner product.

\begin{lemma} \label{lem:projection}
Let $\A \subseteq \F$ be non-empty, closed and convex. For every $f \in \F$ there exists some $a^* \in \A$ such that 
\[ \big \| f - a^* \big \|_{\D_x}^2 = \inf_{a \in \A} \big \| f - a \big \|_{\D_x}^2. \]
Each $a^* \in \A$ that suffices this property, is called an orthogonal projection of $f$ on $\A$.
\end{lemma}

\begin{proof}
By definition of the infimum there exists a sequence $(a_n)_{n=1}^\infty$ in $\A$, such that
\[ \lim_{n \to \infty} \big \| f - a_n \big \|_{\D_x}^2 = \inf_{a \in \A} \big \| f - a \big \|_{\D_x}^2 =: \delta. \]
We show that $(a_n)_{n=1}^\infty$ is a Cauchy sequence. For this we let $n,m \in \N$ and observe that
\[ \begin{split}
\big \| a_n - a_m \big \|_{\D_x}^2 
&= \big \| (f-a_n) - (f-a_m) \big \|_{\D_x}^2 \\\
&= \big \| f - a_n \big \|_{\D_x}^2 + \big \| f - a_m \big \|_{\D_x}^2 - 2 \big \langle f-a_n, f-a_m \big \rangle_{\D_x}.
\end{split} \]
Furthermore we have
\[ \big \| (f-a_n) + (f-a_m) \big \|_{\D_x}^2 = \big \| f - a_n \big \|_{\D_x}^2 + \big \| f - a_m \big \|_{\D_x}^2 + 2 \big \langle f-a_n, f-a_m \big \rangle_{\D_x}. \]

Combining both expressions yields
\[ \begin{split}
\big \| a_n - a_m \big \|_{\D_x}^2 
&= 2 \big \| f - a_n \big \|_{\D_x}^2 + 2 \big \| f - a_m \big \|_{\D_x}^2 - \big \| (f-a_n) + (f-a_m) \big \|_{\D_x}^2 \\\
&= 2 \big \| f - a_n \big \|_{\D_x}^2 + 2 \big \| f - a_m \big \|_{\D_x}^2 - 4 \big \| f - \frac{1}{2}(a_n+a_m) \big \|_{\D_x}^2.
\end{split} \]

By the definition of $\delta$ we observe that
\[ \forall \epsilon > 0 : \exists N \in \N : \forall n > N : \big \| f - a_n \big \|_{\D_x}^2 < \delta + \epsilon. \]
Furthermore, since $\A$ is convex, $\frac{1}{2}(a_n + a_m) \in \A$ and we derive
\[ \big \| f - \frac{1}{2}(a_n+a_m) \big \|_{\D_x}^2 \geq \inf_{a \in \A} \big \| f - a \big \|_{\D_x}^2 = \delta. \]
Using these two observations, we conclude that
\[ \forall \epsilon > 0 : \exists N \in \N : \forall n > N : \big \| a_n - a_m \big \|_{\D_x}^2 \leq 2(\delta + \epsilon) + 2(\delta + \epsilon) - 4\delta = 4\epsilon \]
Thus $(a_n)_{n=1}^\infty$ is a Cauchy sequence. Since $\F$ is complete and $\A$ is closed, $(a_n)_{n=1}^\infty$ converges to some $a^* \in \A$ with $\big \| f - a^* \big \|_{\D_x}^2 = \delta$, which proofs the lemma.
\end{proof}

Note that since $\| \cdot \|_{\D_x}$ is only a semi-norm and not a norm, the limit of $(a_n)_{n=1}^\infty$ is not unique. Essentially there can exist multiple orthogonal projections from $f$ onto the subset $\A$. \\

Lemma \ref{lem:projection} is a weakening of the Hilbert projection theorem stating that, given a Hilbert space, it even exists a unique orthogonal projection. In our case we lose the uniqueness, because $\langle \cdot, \cdot \rangle_{\D_x}$ is just positive semidefinite. However this is just a side note, since the existence of an orthogonal projection suffices for the further theory. Next we infer a useful corollary, which is notated as lemma 3.3.2 in \cite{FunctionalAnalysis}.

\begin{corollary} \label{cor:projection}
Let $f \in \F$ and $\A \subseteq \F$ be a non-empty and closed subspace. For every $a \in \A$ it holds, that $\langle f - a^* , a \rangle_{\D_x} = 0$, where $a^* \in \A$ denotes an orthogonal projection of $f$ on $\A$.
\end{corollary}

\begin{proof}
Let $a \in \A$ and $\lambda \in \R$. Since $\A$ is a subspace, $a^* + \lambda a \in \A$ and we derive
\[ \big \| (a^* + \lambda a) -f \big \|_{\D_x}^2 \geq  \inf_{a \in \A} \big \| f - a \big \|_{\D_x}^2 = \big \| a^* - f \big \|_{\D_x}^2. \]
Rearranging the inequality yields
\[ \big \| (a^* - f) + \lambda a \big \|_{\D_x} ^2 - \big \| a^* - f \big \|_{\D_x}^2 = 2 \big \langle a^* -f , \lambda a \big \rangle_{\D_x} + \big \| \lambda a \big \|_{\D_x}^2 \geq 0, \]
giving rise to the definition of the non-negative function
\[ \psi : \R \to \R : \lambda \mapsto 2 \lambda \big \langle a^* -f , a \big \rangle_{\D_x} + \lambda^2 \big \| a \big \|_{\D_x}^2. \]
Under the assumption that $\big \langle a^* - f , a \big \rangle_{\D_x} \neq 0$ and $\| a \|_{\D_x}^2 \neq 0$, we derive
\[ \psi \Big ( - \frac{ \big \langle a^* - f , a \big \rangle_{\D_x}}{\| a \|_{\D_x}^2} \Big ) = -2 \cdot \frac{ \big \langle a^* - f , a \big \rangle_{\D_x}^2}{ \| a \|_{\D_x}^2} + \frac{\big \langle a^* - f , a \big \rangle_{\D_x}^2}{ \| a \|_{\D_x}^2} = - \frac{ \big \langle a^* - f , a \big \rangle_{\D_x}^2}{ \| a \|_{\D_x}^2} < 0 \]
and under the assumption that $\big \langle a^* - f, a \big \rangle_{\D_x} \neq 0$ and $\| a \|_{\D_x}^2 = 0$, we derive
\[ \psi \Big ( -  \big \langle a^* - f , a \big \rangle_{\D_x} \Big ) = -2 \cdot \big \langle a^* - f , a \big \rangle_{\D_x}^2 < 0, \]
which is a contradiction to $\psi(\lambda) \geq 0$ for every $\lambda \in \R$. Thus $ \big \langle f - a^* , a \big \rangle_{\D_x} = 0$.
\end{proof}

Finally, the concept of orthogonal projections enables us to construct any $\mu \in \F^*$ based on a representer $d \in \F$. The following lemma is a translation of theorem 3.8.1 in \cite{FunctionalAnalysis} to our setting.

\begin{lemma} \label{lem:riesz}
The map
\[ J: \F \to \F^* : d \mapsto \langle d, \cdot \rangle_{\D_x} \]
is linear and surjective.
\end{lemma}

\begin{proof}
The linearity follows directly from the linearity of $\langle \cdot, \cdot \rangle_{\D_x}$. To prove that $J$ is indeed surjective, we let $\mu \in \F^*$ and need to find some $d \in \F$ such that $\mu = J(d)$. \\

Case $\mu=0$: We can choose $d=0$, with $\mu = 0 = J(d)$. \\

Case $\mu \neq 0$: Due to the linearity of $\mu$ we can normalize and find some $e \in \F$ with $\mu[e] = 1$. Let $N \coloneq \mu^{-1}(0)$ denote the kernel of $\mu$, which is non-empty, closed and convex since $\mu$ is linear and continuous. Thus by Lemma \ref{lem:projection} there exists some $p_e \in N$, such that $p_e$ is an orthogonal projection of $e$ on $N$. Define $f \coloneq e - p_e$, then $f \notin N$ since
\[ \mu[f] = \mu[e] - \mu[p_e] = \mu[e] - 0 = 1. \]
Thus we can choose any $g \in \F$, define $h \coloneq g - \mu[g]f$ and use the linearity of $\mu$ to derive
\[ \mu[h] = \mu \big [g- \mu[g]f \big] = \mu[g] - \mu[g] \mu[f] = \mu[g] - \mu[g] \cdot 1 = 0. \]
This implies $h \in N$, so that $\langle f , h \rangle_{\D_x} = \langle e - p_e, h \rangle_{\D_x} = 0$ by corollary \ref{cor:projection} and we conclude
\[ \langle f, g \rangle_{\D_x} = \langle f , h \rangle_{\D_x} + \langle f , \mu[g]f \rangle_{\D_x} = 0 + \mu[g] \cdot \langle f , f \rangle_{\D_x} = \mu[g] \cdot \| f \|_{\D_x}^2. \]
Now we can assume that $\| f \|_{\D_x}^2 \neq 0$. Otherwise we would have $\langle f, g \rangle_{\D_x} = 0$ for every $g \in \F$ and therefore $f = 0$ which is a contradiction to $f \notin N$. Rearranging terms results in
\[ \mu[g] = \big \langle f / \| f \|_{\D_x}^2 , g \big \rangle_{\D_x} = J \big (f / \| f \|_{\D_x}^2 \big)[g]. \]
Hence we have found $d = f / \| f \|_{\D_x}^2$ with $\mu = J(d)$. This shows surjectivity.
\end{proof}

Thus for each functional $\mu \in \F^*$ there exists $d \in \F$ such that $\mu = \langle d, \cdot \rangle_{\D_x}$. In other words
\[ \F^* = \Big \{ \mu : \F \to \R : f \mapsto \langle d,f \rangle _{\D_x} \mid d \in \F \Big \}. \]

Lemma \ref{lem:riesz} is a weakening of the Riesz representation theorem, stating that any Hilbert space $\H$ is isometric and isomorphic to its dual space $\H^*$ via the map $J:\H \to \H^* : x \mapsto \langle x , \cdot \rangle_\H$. \\

Since $\langle \cdot , \cdot \rangle_{\D_x}$ is only positive semidefinite and not positive definite we lose the property of $J$ being isometric and therefore injective. However, based on the definition of $\D_x$, $J$ maps two elements of $\F$ to the same functional, if and only if they are equal on the data. Since our objective is to minimize the cost functional $C$, which depends on the exact same data, it is no matter which representer is chosen and therefore surjectivity will  be sufficient enough for the further argumentation.

\pagebreak
\subsection{Reproducing Kernel Hilbert Space}

\begin{definition}
A multi-dimensional kernel over $\R^{n_0}$ is a function
\[ K: \R^{n_0} \times \R^{n_0} \to \R^{n_l \times n_l} : (x,x') \mapsto K(x,x'), \]
such that $K(x,x') = K(x',x)\tr $ for any $x,x' \in \R^{n_0}$.
\end{definition}

The following definition from RKHS-5:

\begin{definition}
Let $\N_m \coloneq \{1, \dots, m\}$. A kernel $K: \R^{n_0} \times \R^{n_0} \to \R^{n_l \times n_l}$ is said to be positive semidefinite if, for any $m \in \N$, $\{ x_i \mid i \in \N_m \} \subseteq \R^{n_0}$ and $\{ y_i \mid i \in \N_m \} \subseteq \R^{n_l}$, it holds
\[ \sum_{i,j \in \N_m} y_i \tr K(x_i,x_j)y_j \geq 0. \]
\end{definition}

According to Aronszajn in.. we have denote the following theorem.

\begin{theorem}
Let $K: \R^{n_0} \times \R^{n_0} \to \R^{n_l \times n_l}$ be a positive semidefinite kernel and define
\[ \H_0 \coloneq \text{span} \Big \{ K_xy : \R^{n_0} \to \R^{n_l} : z \mapsto K(z,x)y \mid x \in \R^{n_0}, y \in \R^{n_l} \Big \}. \]
Furthermore define the inner product of $f = \sum_{i \in \N_m}K_{x_i}y_i \in \H_0$ and $g = \sum_{i \in \N_m}K_{\hat{x}_i}\hat{y}_i \in \H_0$ as
\[ \langle f,g \rangle_{\H_K} \coloneq \sum_{i,j \in \N_m} y_i \tr K(x_i, \hat{x}_j) \hat{y}_j, \]
then the closure $\H_K \coloneq \overline{H_0}$ is a Hilbert space with inner product $\langle \cdot, \cdot \rangle_{\H_K}$.
\end{theorem}

\begin{definition}
Let $K: \R^{n_0} \times \R^{n_0} \to \R^{n_l \times n_l}$ be a positive semidefinite kernel. We call $\H_K$ in theorem ... the vector-valued reproducing kernel Hilbert space associated with $K$.
\end{definition}

The naming arises from the fact, that the following reproducing property holds.

\begin{lemma}
For any $f \in \H_K$ the reproducing property holds, that is
\[ \forall x \in \R^{n_0}, y \in \R^{n_l} : \langle f(x), y \rangle = \langle f, K_xy \rangle_{\H_K}. \]
\end{lemma}

\begin{proof}
Let $f = \sum_{i \in \N_m}K_{x_i}y_i \in \H_0$ and $x \in \R^{n_0}, y \in \R^{n_l}$ be arbitrary. We observe that
\[ \langle f(x), y \rangle = \sum_{i \in \N_m} \big ( K(x,x_i)y_i \big ) \tr y = \sum_{i \in \N_m} y_i \tr K(x_i,x)y = \langle f, K_xy \rangle_{\H_K}. \qedhere \]
\end{proof}

\pagebreak
$\b:$ bias vector \\
$\w:$ weight vector \\
- \\
$a:$ element of $\A$ \\
$b:$ batch size \\
$d_{\F}$ pseudo metric on $\F$ \\
$d_{\X}:$ metric on input space \\
$d_{\Y}:$ metric on output space \\
$f:$ real valued function \\
$g:$ real valued function \\
$h:$ real valued function \\
$\tilde{f}:$ neural network architecture \\
$i:$ several indices \\
$k:$ iteration index \\
$l:$ number of layers in neural network \\
$\ell:$ loss function \\
$m:$ real number / number of training samples \\
$m_k:$ vector in Adam \\
$n_0:$ input dimension of neural network \\
$n_l:$ output dimension of neural network \\
$n_x:$ input dimension of layer \\
$n_y:$ output dimension of layer \\
$n:$ number of parameters in neural network \\
$s:$ real valued vector in BFGS \\
$v_k:$ vector in Adam \\
$w:$ parameter vector in neural network \\
$x:$ real valued vector (input) \\
$x':$ real valued vector (input) \\
$y:$ real valued vector (output) \\
$z_{\lambda}:$ real valued vector \\
- \\
$\alpha:$ step size \\
$\beta_1:$ momentum factor \\
$\beta_2:$ momentum factor \\
$\epsilon:$ convergence criterium \\
$\lambda:$ real valued scalar / regularization parameter \\
$\nu:$ neuron \\
$\mu:$ element of the dual space \\
$\phi:$ direction in functional derivative \\
$\psi:$ real valued helper function \\
$\rho:$ real value in BFGS\\
$\sigma:$ activation function \\
- \\
$C:$ cost functional \\
$B:$ inverse hessian matrix approximation \\
$E:$ empirical error \\
$I_n:$ identity matrix \\
$J:$ Riesz mapping \\
$K:$ number of epochs / Kernel \\
$L:$ lipschitz constant \\
$M:$ real valued matrix \\
$R:$ generalization error \\
$V:$ matrix in BFGS \\
$W:$ weight matrix in layer \\
- \\
$\A:$ subset of $\F$ \\
$\F:$ function space \\
$\F^*$ dual space \\
$\F |_{\tilde{f}}:$ set of neural networks with given architecture \\
$\D:$ probability distribution \\
$\D_x:$ marginal probability distribution \\
$\I:$ set of indices \\
$\H:$ hilbert space \\
$\X:$ input space \\
$\Y:$ output space \\
- \\
$\E:$ expected value \\
$\N:$ natural numbers \\
$\R:$ real numbers \\

Check: Notation of functionals / dual elements \\

\pagebreak
\subsection{Kernel Gradient Descent}

Until now we have a theoretical understanding of functional gradient descent in the function space $\F$. However there is not yet a connection to gradient descent in the parameter space. In theory, functional gradient descent is an excellent tool to guarantee convergence but in practice we need the explicit parameter vector $w \in \R^n$ to represent the optimal function due to functional gradient descent. This relationship between functional gradient descent and parameter gradient descent can be established by so called kernel gradient descent. \\

We start by recalling the basic definition of one-dimensional kernels.

\begin{definition}
A one-dimensional kernel over $\R^{n_0}$ is a map 
\[ K: \R^{n_0} \times \R^{n_0} \to \R : (x,x') \mapsto K(x,x').\]
A kernel $K$ is said to be positive definite symmetric if for any $m \in \N$ and $x_1, \dots, x_m \in \R^{n_0}$, the matrix with entries $M_{i,j} = K(x_i,x_j) \in \R^{m \times m}$ is symmetric and positive semidefinite.
\end{definition}

This thesis will work with kernels over the input space $\R^{n_0}$, not only with one-dimensional but with multi-dimensional kernels, which motivates the following definition.

\begin{definition}
A multi-dimensional kernel over $\R^{n_0}$ is a function
\[ K: \R^{n_0} \times \R^{n_0} \to \R^{n_l \times n_l} : (x,x') \mapsto K(x,x'), \]
such that $K(x,x') = K(x',x)\tr $ for any $x,x' \in \R^{n_0}$.
\end{definition}

Based on the marginal distribution $\D_x$, kernels induce symmetric bilinear forms.

\begin{lemma} \label{lem:form}
A kernel $K: \R^{n_0} \times \R^{n_0} \to \R^{n_l \times n_l}$ induces the symmetric bilinear form
\[ \langle \cdot, \cdot \rangle_K : \F \times \F \to \R : (f,g) \mapsto \langle f , g \rangle _K \coloneq \E_{x,x' \sim \D_x} \Big [ f(x)\tr  K(x,x') g(x') \Big ], \]
where $x, x' \in \R^{n_0}$ are drawn independently according to $\D_x$. 
\end{lemma}

\begin{proof}
Let $f_1, f_2, f, g \in \F$ and $\lambda \in \R$. We observe, that \\

(i) $\langle f_1 + f_2 , g \rangle_{K} = \langle f_1 , g \rangle_{K} + \langle f_2 , g \rangle_{K}$:
\[ \begin{split}
\langle f_1 + f_2 , g \rangle_{K}
&= \E_{x,x' \sim \D_x} \Big [ \big ( f_1(x) + f_2(x) \big )\tr K(x,x')g(x') \Big ] \\\
&= \E_{x,x' \sim \D_x} \Big [ f_1(x)\tr K(x,x')g(x') + f_2(x)\tr K(x,x')g(x') \Big ] \\\
&= \E_{x,x' \sim \D_x} \Big [ f_1(x)\tr K(x,x')g(x') \Big ] + \E_{x,x' \sim \D_x} \Big [ f_2(x)\tr K(x,x')g(x') \Big ] \\\
&= \langle f_1 , g \rangle_{K} + \langle f_2 , g \rangle_{K},
\end{split} \]

(ii) $\langle \lambda f , g \rangle_{K} = \lambda \langle f, g \rangle_{K}$:
\[ \begin{split} 
\langle \lambda f , g \rangle_{K} 
&= \E_{x,x' \sim \D_x} \Big [ \lambda f(x)\tr K(x,x')g(x') \Big ] \\\
&= \lambda \cdot \E_{x,x' \sim \D_x} \Big [ f(x)\tr K(x,x')g(x') \Big ] = \lambda \langle f, g \rangle_{K}, 
\end{split} \]

(iii) $\langle f , g \rangle_{K} = \langle g , f \rangle_{K}$:
\[ \begin{split} \langle f , g \rangle_{K} 
&= \E_{x,x' \sim \D_x} \Big [ f(x)\tr K(x,x')g(x') \Big ] \\\
&= \E_{x,x' \sim \D_x} \Big [ f(x)\tr K(x',x)\tr g(x') \Big ] \\\
&= \E_{x,x' \sim \D_x} \Big [ g(x')\tr K(x',x)f(x) \Big ] = \langle g , f \rangle_{K}.
\end{split} \]

Thus $\langle \cdot,\cdot \rangle_{K}$ is indeed symmetric and bilinear.
\end{proof}

In contrast to $\langle \cdot , \cdot \rangle_{\D_x}$, the bilinear map $\langle \cdot , \cdot \rangle_{K}$ is not necessarily positive semidefinite. This can be seen from a simple counterexample. Consider the kernel
\[ K: \R^{n_0} \times \R^{n_0} \to \R^{n_l} : x \mapsto - I_{n_l}, \]
then for $f \equiv (1, \dots, 1)^T$ it holds, that
\[ \langle f , f \rangle_{K} = \E_{x,x' \sim \D_x} \Big [ f(x)\tr K(x,x')f(x') \Big ] = -n_l < 0. \]

Using these bilinear forms, one can generalize the definition of positive definite kernels.

\begin{definition} \label{def:definite}
A kernel $K: \R^{n_0} \times \R^{n_0} \to \R^{n_l \times n_l}$ is called positive definite with respect to the seminorm $\| \cdot \| _{\D_x}$ if for every $f \in \F$ it holds, that $\| f \|_{\D_x} > 0$ implies $\langle f, f \rangle_K > 0$.
\end{definition}

We can use positive definite kernels to equip $\F$ with another seminorm.

\begin{lemma}
Given a kernel $K: \R^{n_0} \times \R^{n_0} \to \R^{n_l \times n_l}$, that is positive definite with respect to the seminorm $\| \cdot \|_{\D_x}$ we can use the bilinear form $\langle \cdot, \cdot \rangle_K$ to define another seminorm
\[ \| \cdot \|_K: \F \to \R : f \mapsto \sqrt{\langle f, f \rangle_K}.  \]
\end{lemma}

\begin{proof}
Since $\langle \cdot, \cdot \rangle_K$ is symmetric by lemma \ref{lem:form} it remains to show that $\langle \cdot, \cdot \rangle_K$ is positive semidefinite. For this we let $f \in \F$ be arbitrary and investigate the following cases. \\

Case $\| f \|_{\D_x} > 0$: By definition \ref{def:definite} it holds, that $\langle f, f \rangle_K > 0$. \\

Case $\| f \|_{\D_x} = 0$: By definition of $\langle \cdot , \cdot \rangle_{\D_x}$, we observe that
\[ \big \| f \big \|_{\D_x}^2 = \langle f , f \rangle_{\D_x} = \E_{x \sim \D_x} \Big [ f(x)\tr f(x) \Big ] = \E_{x \sim \D_x} \Big [ \sum_{i=1}^{n_l} f_i(x)^2 \Big ] = 0. \]
Thus for every $x \sim \D_x$ it holds that $f(x) = 0$. Therefore we conclude
\[ \langle f , f \rangle_{K} = \E_{x,x' \sim \D_x} \Big [ f(x)\tr K(x,x')f(x') \Big ] = 0. \]
In total we have shown that $\langle f, f \rangle_K \geq 0$ for every $f \in \F$, such that $\| \cdot \|_K$ is a seminorm.
\end{proof}

This concludes the necessary knowledge on kernels. Next we will introduce the kernel gradient of a functional, which is based on the partial application of kernels.

\begin{definition}
Let $K: \R^{n_0} \times \R^{n_0} \to \R^{n_l \times n_l}$ be a kernel. For $i=1, \dots, n_l$ one defines the partial application of $K$ as the function
\[ K_{\cdot,i} : \R^{n_0} \times \R^{n_0} \to \R^{n_l} : (x,x') \mapsto K(x,x')_{\cdot,i}, \]
where $K(x,x')_{\cdot,i}$ denotes the $i$-th column of the $n_l \times n_l$ matrix $K(x,x')$.
\end{definition}

Fixing one argument of $K_{\cdot,i}$ results in a function in $\F$. This enables the following definition.

\begin{definition}
Given a kernel $K: \R^{n_0} \times \R^{n_0} \to \R^{n_l \times n_l}$, we define the map
\[ \Phi_K : \F^* \to \F : \mu \mapsto f_{\mu}, \]
mapping a dual element $\mu \in \F^*$ to the function
\[ f_{\mu} : \R^{n_0} \to \R^{n_l} : x \mapsto \Big ( \mu \big [ K_{\cdot,1}(\cdot,x) \big ], \dots, \mu \big [ K_{\cdot,n_l}(\cdot,x) \big ] \Big ) \tr. \]
\end{definition}

Based on the definition of $\Phi_K$ we can define the kernel gradient of a functional.

\begin{definition}
Let $f_0 \in \F$, $\mu : \F \to \R$ be a differentiable functional with $\partial_f\mu|_{f_0} \in \F^*$ and $K$ be a kernel. The kernel gradient of $\mu$ at $f_0$ with respect to $K$ is defined as
\[ \nabla_K\mu|_{f_0}: \R^{n_0} \to \R^{n_l} : x \mapsto \Phi_K \Big ( \partial_f\mu|_{f_0} \Big )(x). \]
\end{definition}

Using this definition, we can formulate the kernel gradient descent method.

\begin{algorithm}
\caption{Kernel Gradient Descent} \ \\
\textcolor{white}{$\Big |$}1: Requires kernel $K$, differentiable $\mu: \F \to \R$ with $\partial_f\mu|_{f_0} \in \F^*$, random $f_0 \in \F$ and $\epsilon \geq 0$. \\
\textcolor{white}{$\Big |$}2: Initialize $k \leftarrow 0$. \\
\textcolor{white}{$\Big |$}3: \textbf{while} $\big \| \nabla_K \mu |_{f_k} \big \|_{\D_x} > \epsilon$ \textbf{do}: \\
\textcolor{white}{$\Big |$}4: \quad Choose a step size $\alpha_k > 0$. \\
\textcolor{white}{$\Big |$}5: \quad Set $f_{k+1} \leftarrow f_k - \alpha_k \cdot \nabla_ K \mu |_{f_k}$ and $k \leftarrow k+1$. \\
\textcolor{white}{$\Big |$}6: \textbf{end while}
\end {algorithm}

In the following, we will apply kernel gradient descent to minimize the cost $C$. During this we are especially interested in the evolution of the parameters defining the sequence $(f_k)_{k \in \N_0}$. \\

With the choice of a differentiable loss function, the cost functional $C : \F \to \R$ is differentiable as well, such that the functional derivative $\partial_{f} C |_{f_0}$ is well-defined for every $f_0 \in \F$. \\

In the following we will make the restriction, that the marginal distribution $\D_x$ on the input space $\R^d$ is given by the empirical distribution on a finite subset of $\R^d$. This means there exist $m \in \N$ and $x_1, \dots, x_m \in \R^d$ such that 
\[ \D_x = \frac{1}{m} \sum_{i=1}^{m} \delta_{x_i}, \]
where $\delta_{x_i}$ denotes the Dirac measure in $x_i$ for $i=1,\dots,m$. Under this assumption, the cost $C$ depends only on the values of $f$ at a finite dataset. Thus we conclude, that $\partial_{f} C |_{f_0}$ is linear and continuous and therefore $\partial_{f} C |_{f_0} \in \F^*$ for every $f_0 \in \F$. Hence by Lemma \ref{lem:riesz} there exists a representer $d|_{f_0} \in \F$, such that the functional derivative of $C$ can be written as
\[ \partial_{f} C |_{f_0} = \big \langle d|_{f_0}, \cdot \big \rangle_{\D_x}. \]

Next we will use this restriction on the distribution $\D_x$, to simplify the notation of $\Phi_K$.

\begin{lemma} \label{lem:phi}
Let $\mu = \langle d, \cdot \rangle_{\D_x} \in \F^*$ and $K: \R^d \times \R^d \to \R^{p \times p}$ be a kernel, then
\[ \Phi_K(\mu) = \frac{1}{m}\sum_{i=1}^{m} K(\cdot,x_i)d(x_i), \]
where $x_1, \dots, x_m \in \R^d$ denotes the finite support of the empiricial distribution $\D_x$.
\end{lemma}

\begin{proof}
Let $x \in \R^d$ be arbitrary. By the definition of $\Phi_K$ we have
\[ \Phi_K \big ( \mu \big )(x) = \Phi_K \Big ( \langle d, \cdot \rangle_{\D_x} \Big )(x) = \begin{bmatrix} \mu [K_{\cdot,1}(\cdot,x) ] \\ \vdots \\ \mu [ K_{\cdot,p}(\cdot,x)] \end{bmatrix} = \begin{bmatrix} \big \langle d, K_{\cdot,1}(\cdot,x) \big \rangle_{\D_x} \\ \vdots \\ \big \langle d, K_{\cdot,p}(\cdot,x) \big \rangle_{\D_x} \end{bmatrix}. \]
Recalling the definition of $\langle \cdot , \cdot \rangle_{\D_x}$, it holds that
\[ \big \langle d, K_{\cdot,j}(\cdot,x) \big \rangle_{\D_x} = \E_{x' \sim \D_x} \Big [ d(x')\tr K_{\cdot,j}(x',x) \Big ] \]
for $j = 1, \dots, p$. Thus we derive
\[ \Phi_K \big ( \mu \big )(x) = \begin{bmatrix} \big \langle d, K_{\cdot,1}(\cdot,x) \big \rangle_{\D_x} \\ \vdots \\ \big \langle d, K_{\cdot,p}(\cdot,x) \big \rangle_{\D_x} \end{bmatrix} = \E_{x' \sim \D_x} \begin{bmatrix} d(x')\tr K_{\cdot,1}(x',x) \\ \vdots \\ d(x')\tr K_{\cdot,p}(x',x) \end{bmatrix} = \E_{x' \sim \D_x} \Big [ d(x')\tr  K(x',x) \Big ]\tr . \]
With $K(x',x)\tr  = K(x,x')$ for every $x,x' \in \R^d$ by definition, we conclude
\[ \Phi_K \big ( \mu \big )(x) = \E_{x' \sim \D_x} \Big [ d(x')\tr  K(x',x) \Big ]\tr  = \E_{x' \sim \D_x} \Big [ K(x',x)\tr d(x') \Big ] = \E_{x' \sim \D_x} \Big [ K(x,x')d(x') \Big ]. \]
The last step consists of using the assumption, that $\D_x$ is the empirical distribution on a finite dataset $x_1, \dots, x_m \in \R^d$. This enables us to derive
\[ \Phi_K \big ( \mu \big )(x) = \E_{x' \sim \D_x} \Big [ K(x,x')d(x') \Big ] = \frac{1}{m}\sum_{i=1}^{m} K(x,x_i)d(x_i). \]
This proves the statement of the lemma.
\end{proof}

By lemma \ref{lem:phi} we can write the kernel gradient of $C$ at $f_0$ with respect to some kernel $K$ as
\[ \nabla_KC|_{f_0}(x) = \Phi_K \Big ( \big \langle d|_{f_0} , \cdot \big \rangle_{\D_x} \Big )(x) = \frac{1}{m} \sum_{i=1}^{m} K(x,x_i)d|_{f_0}(x_i). \]

Due to the definition of the kernel, $\nabla_KC|_{f_0}$ has the advantage of being well-defined on the whole input space $\R^d$, where as the partial derivative $\partial_fC|_{f_0} =\langle d|_{f_0}, \cdot \rangle_{\D_x}$ is based on the bilinear form $\langle \cdot, \cdot \rangle_{\D_x}$ and therefore only defined on the finite dataset.

\begin{definition}
Let $K$ be a kernel. A time dependent function $f : \R \to \F$ is said to follow the kernel gradient descent on $C$ with respect to K if it satisfies the differential equation
\[ \partial_tf(t) = - \nabla_KC|_{f(t)}. \]
\end{definition}

We can think of such a function as a continuous extension to the sequence produced by kernel gradient descent on $C$, which can be used to investigate the change in $C$.

\begin{lemma}
Let $K$ be a positive definite kernel with respect to $\| \cdot \|_{\D_x}$ and $f: \R \to \F$ be a time dependent function, that follows the kernel gradient descent on $C$ with respect to $K$, then the evolution of $C \circ f$ during kernel gradient descent is expressed by
\[ \partial_tC|_{f(t)} = - \big \| d|_{f(t)} \big \|_{K}^2, \]
where $d|_{f(t)} \in \F$ denotes the representer of $\partial_f C|_{f(t)} = \big \langle d|_{f(t)}, \cdot \big \rangle_{\D_x} \in \F^*$ for $t \in \R$.
\end{lemma}

\begin{proof}
Using the notation $C|_{f(t)} = C \big [f(t) \big ]$, we derive
\[ \partial_tC|_{f(t)} = \partial_f C|_{f(t)} \big [ \partial_t f(t) \big ] = \big \langle d|_{f(t)}, \cdot \big \rangle_{\D_x} \big [ \partial_t f(t) \big ] = \big \langle d|_{f(t)}, \partial_t f(t) \big \rangle_{\D_x}. \]
Since $f$ follows the kernel gradient descent on $C$ with respect to $K$ this can be rewritten as
\[ \partial_tC|_{f(t)} = \big \langle d|_{f(t)}, \partial_t f(t) \big \rangle_{\D_x} = \big \langle d|_{f(t)}, - \nabla_K C|_{f(t)} \big \rangle_{\D_x}. \]
Recalling the definition of $\nabla_K C|_{f(t)}$, which was given by
\[ \nabla_KC|_{f(t)} = \frac{1}{m} \sum_{i=1}^{m} K(\cdot,x_i)d|_{f(t)}(x_i), \]
we derive by the linearity of $\langle \cdot, \cdot \rangle_{\D_x}$, that
\[ \begin{split} 
\big \langle d|_{f(t)}, - \nabla_K C|_{f(t)} \big \rangle_{\D_x} 
&= - \frac{1}{m} \sum_{i=1}^{m} \big \langle d|_{f(t)}, K(\cdot,x_i)d|_{f(t)}(x_i) \big \rangle_{\D_x} \\\
&= - \frac{1}{m} \sum_{i=1}^{m} \E_{x \sim \D_x} \Big [ d|_{f(t)}(x)\tr K(x,x_i)d|_{f(t)}(x_i) \Big ] \\\
&= - \E_{x' \sim \D_x} \E_{x \sim \D_x} \Big [ d|_{f(t)}(x)\tr K(x,x')d|_{f(t)}(x') \Big ]
= - \big \langle d|_{f(t)}, d|_{f(t)} \big \rangle_{K}.
\end{split} \]
Finally we conclude, that
\[ \partial_tC|_{f(t)} = \big \langle d|_{f(t)}, - \nabla_K C|_{f(t)} \big \rangle_{\D_x} = - \big \langle d|_{f(t)}, d|_{f(t)} \big \rangle_{K} = - \big \| d|_{f(t)} \big \|_{K}^2. \]
This completes the proof.
\end{proof}

The lemma guarantees the convergence of $C$ to a critical point, as $\partial_tC|_{f(t)} \leq 0$. In the case where $C$ is convex and bounded from below, this implies the convergence of $f(t)$ to a global minimum $f^* \in \F$ for $t \to \infty$.

\subsection{Kernel Approximation} \label{sec:approximation}

This subsection illustrates the relationship between gradient descent in the parameter space and kernel gradient descent in the function space under the assumption, that the objective function depends linear on its parameters. The example given in here, will be generalized later on, leading to the definition of the neural tangent kernel. \\

To start things off, we let $\D_{\F}$ be any probability distribution on the function space $\F$ and construct a kernel $K$, such that for $x,x' \in \R^d$ the image $K(x,x')$ is a $p \times p$ matrix defined by
\[ K_{i,j}(x,x') \coloneq \E_{f \sim \D_{\F}} \Big [ f_i(x) \cdot f_j(x) \Big ], \quad i,j=1,\dots,p. \]
We will see, that this kernel can be approximated by random functions. For this we choose some $n \in \N$ and draw $n$ random functions $f^{(k)} : \R^d \to \R^p$ for $k=1,\dots,n$ independently from the distribution $\D_{\F}$. Using these, we can define the linear map
\[ F^\textit{lin}: \R^n \to \F : w \mapsto f_w \coloneq \frac{1}{\sqrt{n}} \sum_{k=1}^{n}w_kf^{(k)}, \]
whose partial derivatives for $k=1,\dots,n$ are given by
\[ \partial_{w_{k}}F^\textit{lin} : \R^n \to \F: w \mapsto \frac{1}{\sqrt{n}}f^{(k)}. \]

Based on $F^\textit{lin}$ we can define another kernel, that will be used for the approximation of $K$.

\begin{definition}
For $x \in \R^p$ we define the symmetric matrix $x \otimes x \coloneq M \in \R^{p \times p}$, where
\[ M_{i,j} \coloneq x_i \cdot x_j, \quad  i,j=1,\dots,p. \]
\end{definition}

\begin{definition}
Given $n$ random functions $f^{(k)} : \R^d \to \R^p$ for $k=1,\dots,n$ drawn i.i.d. according to some distribution $\D_{\F}$ on the function space $\F$, the tangent kernel is defined as
\[ \tilde{K} \coloneq \sum_{k=1}^{n} \partial_{w_k} F^\textit{lin}(w) \otimes \partial_{w_k} F^\textit{lin}(w) = \frac{1}{n} \sum_{k=1}^{n} f^{(k)} \otimes f^{(k)}. \]
\end{definition}

In other words, $\tilde{K}$ maps a tuple $(x,x') \in \R^d \times \R^d$ to the matrix $\tilde{K} \in \R^{p \times p}$ with entries 
\[ \tilde{K}_{i,j}(x,x') = \frac{1}{n} \sum_{k=1}^{n} f_i^{(k)}(x) \cdot f_j^{(k)}(x'), \quad  i,j=1,\dots,p. \]

Similar to the definition of time dependent functions, that follow the kernel gradient descent on $C$ with respect to some kernel $K$ we can define time dependent functions, that follow the gradient descent on $C \circ F^\textit{lin}$. This is done by the following definition.

\begin{definition} \label{def:follow}
A time dependent function $w : \R \to \R^n$ is said to follow the gradient descent on $C \circ F^\textit{lin}$, if it satisfies the differential equation
\[ \partial_tw(t) = - \nabla \big (C \circ F^\textit{lin} \big ) (w(t)) = - \nabla C|_{f_{w(t)}}, \]
where $- \nabla C|_{f_{w(t)}}$ denotes the gradient of $C \circ F^\textit{lin}$ at $w(t)$.
\end{definition}

Again, we can think of such a function as a continuous extension to the sequence produced by gradient descent on $C \circ F^\textit{lin}$.

\begin{lemma} \label{lem:evolution}
Let $w: \R \to \R^n$ be a time dependent function, that follows the gradient descent on $C \circ F^\textit{lin}$. For $k=1, \dots, n$, the change in $w_k$ during gradient descent is given by
\[ \partial_tw_k(t) = - \frac{1}{\sqrt{n}} \big \langle d|_{f_{w(t)}}, f^{(k)} \big \rangle_{\D_x}, \]
where $d|_{f_{w(t)}} \in \F$ denotes the representer of $\partial_f C|_{f_{w(t)}} = \big \langle d|_{f_{w(t)}}, \cdot \big \rangle_{\D_x} \in \F^*$ for $t \in \R$.
\end{lemma}

\begin{proof}
Using the notation in definition \ref{def:follow}, we derive
\[ \partial_tw_k(t) = - \partial_{w_{k}} \big ( C \circ F \big )(w(t)) = \partial_fC|_{f_{w(t)}} \Big [ - \frac{1}{\sqrt{n}} f^{(k)} \Big ] = - \frac{1}{\sqrt{n}} \big \langle d|_{f_{w(t)}}, f^{(k)} \big \rangle_{\D_x} \]
for $k = 1, \dots, n$. This completes the proof.
\end{proof}

Based on this lemma we can establish a relationship between time dependent functions that follow the gradient descent on $C \circ F^\textit{lin}$ and time dependent functions that follow the kernel gradient descent on $C$ with respect to the tangent kernel.

\begin{lemma}
Let $w: \R \to \R^n$ be a time dependent function, that follows the gradient descent on $C \circ F^\textit{lin}$, then the time dependent function $F^\textit{lin} \circ w : \R \to \F : t \mapsto f_{w(t)}$ follows the kernel gradient descent on $C$ with respect to the tangent kernel $\tilde{K}$. Formally this is
\[ \partial_t \big (F^\textit{lin} \circ w \big )(t) = \partial_tf_{w(t)} = -\nabla_{\tilde{K}}C|_{f_{w(t)}}. \]
\end{lemma}

\begin{proof}
By lemma \ref{lem:evolution} and the definition of $F^\textit{lin}$ it holds, that 
\[ \partial_tf_{w(t)} = \frac{1}{\sqrt{n}} \sum_{k=1}^{n} \partial_t w_k(t) \cdot f^{(k)} = - \frac{1}{n} \sum_{k=1}^{n} \big \langle d|_{f_{w(t)}}, f^{(k)} \big \rangle_{\D_x} \cdot f^{(k)}. \]
Furthermore, we can write the kernel gradient of $C$ at $f_{w(t)}$ with respect to $\tilde{K}$ as
\[ \begin{split}
\nabla_{\tilde{K}} C|_{f_{w(t)}}
&= \frac{1}{m} \sum_{i=1}^{m} K(\cdot,x_i) \cdot d|_{f_{w(t)}}(x_i) \\\
&= \frac{1}{m} \sum_{i=1}^{m} \bigg [ \frac{1}{n} \sum_{k=1}^{n}f^{(k)} \otimes f^{(k)} \bigg ] (\cdot,x_i) \cdot d|_{f_{w(t)}}(x_i) \\\
&= \frac{1}{n} \sum_{k=1}^{n} \bigg [ \frac{1}{m} \sum_{i=1}^{m} f^{(k)}(x_i)	\tr  d|_{f_{w(t)}}(x_i) \bigg ] \cdot f^{(k)} \\\
&= \frac{1}{n} \sum_{k=1}^{n} \E_{x \sim \D_x} \Big [ f^{(k)}(x)\tr  d|_{f_{w(t)}}(x) \Big ] \cdot f^{(k)} \\\
&= \frac{1}{n} \sum_{k=1}^{n} \big \langle f^{(k)}, d|_{f_{w(t)}} \big \rangle_{\D_x} \cdot f^{(k)}.
\end{split} \]
Comparison of the previous two expressions provides
\[ \partial_tf_{w(t)} = - \frac{1}{n} \sum_{k=1}^{n} \big \langle d|_{f_{w(t)}}, f^{(k)} \big \rangle_{\D_x} \cdot f^{(k)} = -\nabla_{\tilde{K}} C|_{f_{w(t)}}. \]
This proves, that $F^\textit{lin} \circ w$ follows the kernel gradient descent on $C$ with respect to $\tilde{K}$.
\end{proof}

Thus kernel gradient descent on $C$ with respect to the tangent kernel $\tilde{K}$ is equivalent to gradient descent on $C \circ F^\textit{lin}$. For $i,j = 1, \dots, p$ the law of large numbers yields
\[ \forall x,x' \in \R^d : \lim_{n \to \infty} \tilde{K}_{i,j}(x,x') = \lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^{n} f_i^{(k)}(x) \cdot f_j^{(k)}(x') = \E_{f \sim \D_{\F}} \Big [ f_i(x) \cdot f_j(x) \Big ]. \]
Hence the random tangent kernel $\tilde{K}$ is an approximation of the constant limiting kernel $K$.

\subsection{Neural Tangent Kernel}

The training of neural networks behaves quite similar to the kernel gradient descent with respect to the tangent kernel. However, in contrast to the situation in section \ref{sec:approximation}, we consider the realization function
\[ F: \R^n \to \tilde{\F} : w \mapsto f(x,w), \]
 which is no longer linear due to the nested construction of neural networks. Therefore, the partial derivatives $\partial_{w_k}F$ are no longer independent of the parameters $w$, which motivates the definition of a new kernel, specifically related to neural networks.

\begin{definition} \label{def:ntk}
The neural tangent kernel is defined as
\[ \Theta_w \coloneq \sum_{k=1}^{n} \partial_{w_k} F(w) \otimes \partial_{w_k} F(w). \]
\end{definition}

Thus, in contrast to the tangent kernel, which is constant during training since it is independent on the parameters $w$, the neural tangent kernel is random at initialization and varies during training. Therefore the relationship between gradient descent in the parameter space and kernel gradient descent with respect to the neural tangent kernel is more complex. \\

We will see, that in the infinite-width limit, that is the number of neurons per layer tends to infinity, the neural tangent kernel becomes deterministic at initialization and stays constant during training. \\

In order to simplify the further notation, we have to make some adjustments to the notion of neural networks in section \ref{sec:notation}. We still consider neural networks for $l \in \N$ and $d_0, \dots, d_l \in \N$ as functions
\[ f: \R^{d_0} \to \R^{d_l} : x \mapsto f_l \circ f_{l-1} \circ \cdots \circ f_1(x), \]
where each $f_i$ for $i=1, \dots, l$ is a layer with input dimension $d_{i-1}$ and output dimension $d_i$. However, in contrast to the notation in section \ref{sec:notation} we denote these layers as functions 
\[ f_{i} : \R^{d_{i-1}} \to \R^{d_i} : x \mapsto \sigma \Big ( \frac{1}{\sqrt{d_{i-1}}} W^{(i-1)}x + \beta b^{(i-1)} \Big ), \]
where the activation function $\sigma$ is applied element-wise. This notation arises by formulating definition \ref{def:layer} in terms of vectors and matrices, and introducing the additional factors $1/\sqrt{d_{i-1}}$ and $\beta > 0$. This procedure is important for later results.

\begin{definition}
Let $T \subset \R$ be an index set. A family of random variables $\{ X_t \mid t \in T \}$ is called a Gaussian process, if and only if for every finite set $\{ t_1, \dots, t_k \} \subset T$, the random variable $(X_{t_1}, \dots, X_{t_k})$ is multivariate Gaussian distributed.
\end{definition}

\begin{lemma}
Let $f: \R^{d_0} \to \R^{d_l}$ be a neural network of depth $l$, with a Lipschitz continuous non-linear activation function $\sigma: \R \to \R$ at each neuron. In the limit as $d_1, \dots, d_{l-1} \to \infty$ sequentially, the output functions $f_i(x,w)$ for $i=1, \dots, d_l$, tend to i.i.d. centered Gaussian processes of covariance $\Sigma^{(l)}$, where $\Sigma^{(l)}$ is defined recursively by
\[ \begin{split}
\Sigma^{(l)}(x,x') &= \frac{1}{d_0} x\tr x' + \beta^2 \\\
\Sigma^{(l+1)}(x,x') &= \E_{f \sim \mathcal{N}(0, \Sigma^{(l)})} \Big [ \sigma \big ( f(x) \big ) \sigma \big ( f(x') \big) \Big ] + \beta^2,
\end{split} \]
taking the expectation with respect to a centered Gaussian process $f$ of covariance $\Sigma^{(l)}$.
\end{lemma}

\begin{proof}
\textbf{TO DO ---------------------------------------------------------------------------------}
\end{proof}

\begin{theorem}
Let $f: \R^{d_0} \to \R^{d_l}$ be a neural network of depth $l$, with a Lipschitz continuous non-linear activation function $\sigma: \R \to \R$ at each neuron. In the limit as $d_1, \dots, d_{l-1} \to \infty$ sequentially, the neural tangent kernel $\Theta_w$ converges in probability to a deterministic limiting kernel, this is
\[ \Theta_w \to \Theta_{\infty} \otimes I_{d_0}. \]
The scalar kernel $\Theta_{\infty} : \R^{d_0} \times \R^{d_0} \to \R$ is defined recursively by
\[ \begin{split} 
\Theta_{\infty}^{(1)} (x,x') &= \Sigma^{(1)}(x,x') \\\
\Theta_{\infty}^{(l+1)}(x,x') &= \Theta_{\infty}^{(l)}(x,x') \dot{\Sigma}^{(l+1)}(x,x') + \Sigma^{(l+1)}(x,x'),
\end{split} \]
where 
\[ \dot{\Sigma}^{(l+1)}(x,x') \coloneq \E_{f \sim \mathcal{N}(0, \Sigma^{(l)})} \Big [ \sigma' \big ( f(x) \big ) \sigma' \big ( f(x') \big) \Big ], \]
taking the expectation with respect to a centered Gaussian process $f$ of covariance $\Sigma^{(l)}$.
\end{theorem}

\begin{proof}
\textbf{TO DO ---------------------------------------------------------------------------------}
\end{proof}

\begin{theorem}
Assume that $\sigma : \R \to \R$ is a Lipschitz continuous and twice differentiable non-linear function with bounded second derivative. For any $T$ such that the integral $\int_{0}^{T} \| d_t \|_{\D_x} \ dt$ stays stochastically bounded, as $d_1, \dots, d_{l-1} \to \infty$ sequentially, we have, uniformly for $t \in [0,T]$, that
\[ \Theta_w \to \Theta_{\infty} \otimes I_{d_l}. \]
As a consequence, in this limit, the dynamics of $f_w$ is described by the differential equation
\[ \partial_tf_{w(t)} = \Phi_{\Theta_{\infty} \otimes I_{d_l}} \Big ( \big \langle d_t, \cdot \big \rangle_{\D_x} \Big ). \]
\end{theorem}

\begin{proof}
\textbf{TO DO ---------------------------------------------------------------------------------}
\end{proof}

\begin{theorem} \label{thm:stieltjes}
Theorem 3.7 from \cite{Stieltjes}
\end{theorem}

\begin{proof}
\textbf{TO DO ---------------------------------------------------------------------------------}
\end{proof}

\begin{theorem} \label{thm:linear}
Theorem 2.1 from \cite{Linear}
\end{theorem}

\begin{proof}
\textbf{TO DO ---------------------------------------------------------------------------------}
\end{proof}

\pagebreak
\textbf{To Do} \\
Lemma 3.5: check pseudometric properties \\
Lemma 3.5: confirm completeness argument \\
Lemma 3.8: explain why we need this lemma \\

\textbf{TO DO - Section 3.2} \\
\textbf{LEONARDO:} Why is $\partial_{f} C |_{f_0}$ linear and continuous? \\
\textbf{LEONARDO:} Why does the first equality in proof of lemma 3.20 holds? \\

\textbf{TO DO - Section 3.3} \\
\textbf{LEONARDO:} Why does the first equality in proof of lemma 3.24 holds? \\
\textbf{LEONARDO:} Is the equivalence between the gradient methods clear? \\

\textbf{TO DO} \\
References (E-Mail + 3.7 wikipedia references) \\
Notation (double variables) \\
Notation 3.7: d instead of $\mu$ \\

\textbf{TO DO MORE DETAILED} \\
Completeness argument \\
Critical point of functionals \\
Convergence of kernel gradient descent \\
More general words on why we need theorems and definitions \\

\textbf{TO DO POTENTIALLY} \\
Proofs 3.29 - 3.31 \\
Backpropagation \\
Convergence of functional gradient descent \\

\textbf{TIPS} \\
libgen.rs for sources \\

\pagebreak
\section{Training in Low-Dimensional Subspaces}

This section is dedicated to the theoretical considerations on how to efficiently reduce the computation effort needed, to train deep neural networks. If not otherwise specified, the following results stem from \cite{Paper}. The authors approach is based on the hypothesis, that deep neural networks can be trained in low-dimensional subspaces. They argue, that the parameter optimization trajectory could be embedded in a tiny subspace. We will start by motivating this hypothesis based on the previous seen properties of the neural tangent kernel. Afterwards we will introduce an efficient method for the subspace extraction, that enables us to train the network in a tiny subspace. Finally we can construct a second order optimization method training the parameters in the lower-dimensional space.

\subsection{Approach}

In the following we will denote the parameter training sequence by $(w_k)_{k=0, \dots, K}$ after $K \in \N$ iterations. This sequence naturally arises during any of the proposed optimization methods in section \ref{sec:Training}. The low-dimensional landscape hypothesis from \cite{Paper} assumes that, there exists a low-dimensional affine set which approximately contains the optimization trajectory. This can be simply illustrated by the following example.

\begin{figure}[!h]
\centering
\includegraphics[width=0.45\linewidth]{images/trajectory.png}
\caption{Low-dimensional parameter trajectory}
\label{fig:trajectory}
\end{figure}  

Figure \ref{fig:trajectory} visualizes a three dimensional space containing the three optimizable parameters $w(1), w(2), w(3)$. However, the training trajectory $(w_k)_{k=0, \dots, 3}$ could be embedded in a two-dimensional hyperplane, spanned by the orthogonal vectors $e_1$ and $e_2$. Note that none of the optimizable variables could be left out to reduce the dimension, but rather one has to construct new independent variables to obtain the lower-dimensional subspace that approximately contains the optimization trajectory. \\

In the following we will investigate this hypothesis from a theoretical background and conduct several experiments indicating that deep neural networks can be well trained in tiny subspaces of approximately dimension 50. This enables us to efficiently use second order information in the training process and improves robustness against label noise. 

\subsection{Theory}

To effectively reduce the dimensionality we need some background knowledge on the singular value decomposition (SVD) and low-rank approximations.

\subsubsection{Singular Value Decomposition}

The following result is denoted and proven as theorem 2.5.2 in \cite{SVD}.

\begin{theorem}[Singular Value Decomposition] \label{thm:svd}
For every matrix $A \in \R^{m \times n}$ there exist orthogonal matrices $U \in \R^{m \times m}$ and $V \in \R^{n \times n}$ and a diagonal matrix 
\[ \Sigma \coloneq \text{diag}(\sigma_1, \dots, \sigma_p) \in \R^{m \times n} \]
with $p \coloneq \text{min} \{ m,n \}$ and singular values $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_p \geq 0$, such that
\[ A = U \Sigma V\tr . \]
\end{theorem}

The SVD decomposes any real valued matrix in a product of three special matrices. This representation yields the singular values which, like eigenvalues, characterize important properties of a matrix. For the further course we need the following definition.

\begin{definition}
The columns of the matrix $U = [u_1, \dots, u_m]$ are referred to as left singular vectors and the columns of the matrix $V = [v_1, \dots, v_n]$ are referred to as right singular vectors.
\end{definition}

Theorem \ref{thm:svd} shows that the sequence of singular values is unique, since they are sorted in descending order. However, the matrices $U$ and $V$ do not have to be unique. For example, the corresponding singular vectors can be interchanged if there are two identical singular values. Therefore, a matrix can have several singular value decompositions. \\

As a simple implication of theorem \ref{thm:svd} we denote the following corollary.

\begin{corollary} \label{cor:svd}
Let $A \in \R^{m \times n}$ be a matrix with singular value decomposition $A=U \Sigma V\tr $, such that $p \coloneq \text{min}\{m,n\}$ and $\sigma_1 \geq \dots \geq \sigma_r > \sigma_{r+1} = \dots = \sigma_p = 0$ for $r \leq p$.
\begin{itemize}
\item[1.] Denoting the columns of $U$ and $V$ with $u_i$ and $v_i$, we have
\[ A v_i = \sigma_i u_i, \quad A\tr  u_i = \sigma_i v_i, \quad i=1,\dots,p.\]
\item[2.] The squares of the singular values $\sigma_1^2, \dots, \sigma_r^2$ are the eigenvalues of $A\tr A$ to the corresponding eigenvectors $v_1, \dots, v_r$.
\end{itemize}
\end{corollary}

\begin{proof}
\ 
\begin{itemize}
\item[1.] Using $A=U \Sigma V\tr $ and the orthogonality of $V$ we derive
\[ Av_i = U \Sigma V\tr  v_i = U \Sigma e_i = U \sigma_i e_i = \sigma_i u_i \]
for $i=1, \dots, p$. Analogously we conclude the second statement
\[ A\tr  u_i = V \Sigma U\tr  u_i = V \Sigma e_i = V \sigma_i e_i = \sigma_i v_i. \]
\item[2.] Since $A\tr A$ is symmetric, its spectral decomposition is given by
\[ A\tr A = V \Sigma\tr  U\tr  U \Sigma V\tr  = V \Sigma\tr  \Sigma V\tr  = V \Sigma\tr  \Sigma V^{-1}. \]
Using $\Sigma\tr  \Sigma = \text{diag}(\sigma_1^2, \dots, \sigma_r^2, 0, \dots, 0) \in \R^{n \times n}$ we conclude the corollary. \qedhere
\end{itemize}
\end{proof}

The SVD is an excellent tool to find low-rank approximations of a matrix $A \in \R^{m \times n}$, that is to find a matrix $B \in \R^{m \times n}$ with $\text{rank}(B) < \text{rank}(A)$ such that $\|A -B\|$ is small for some given matrix norm. We can generate such a low-rank approximation by simply reducing the number of singular values in the singular value decomposition.

\begin{definition}
Let $A \in \R^{m \times n}$ be a matrix with singular value decomposition $A=U \Sigma V\tr $. For $k < r = \text{rank}(A)$ we define an approximation of $A$ as
\[ A^k \coloneq \sum_{i=1}^{k} u_i \sigma_i v_i\tr  = \tilde{U} \tilde{\Sigma} \tilde{V}\tr , \]
where $\tilde{U} \coloneq [u_1, \dots, u_k]$, $\tilde{V} \coloneq [v_1, \dots, v_k]$ and $\tilde{\Sigma} \coloneq \text{diag}(\sigma_1, \dots, \sigma_k)$.
\end{definition}

In terms of the spectral norm $\| \cdot \|_2$, the matrix $A^k$ is the best possible rank-$k$ approximation. The underlying theorem is the following, which is denoted and proven as theorem 2.5.3 in \cite{SVD}.

\begin{theorem}[Eckart-Young-Mirsky] \label{thm:eym}
Let $A \in \R^{m \times n}$ be a matrix with singular value decomposition $A= U \Sigma V\tr $. For every $k < r = \text{rank}(A)$ it holds, that
\[ \min_{\text{rank}(B)=k} \big \| A-B \big \|_2 = \big \| A - A^k \big \|_2 = \sigma_{k+1}. \]
\end{theorem}

This concludes the necessary background knowledge on singular value decompositions and low-rank approximations, which will be useful for the subspace extraction.

\subsubsection{Dynamic Linear Dimensionality Reduction}

Next, we investigate a single output neural network architecture 
\[ f: \R^d \times \R^n \to \R : (x,w) \mapsto f(x,w). \]
As seen in section \ref{sec:Training} the neural network architecture $f$ induces a function space
\[ \F \coloneq \Big \{ f_w : \R^d \to \R : x \mapsto f(x,w) \mid w \in \R^n \Big \}. \]
In order to train the neural network we let
\[ \X = \big \{ (x_i,y_i) \mid i=1, \dots, m \big \} \subset \R^d \times \R \] 
denote a training set of size $m \in \N$. Recall that training of $f$ refers to finding the optimal parameter vector $w$, which is accomplished by minimizing the empirical error function
\[ E : \R^n \to \R_+ : w \mapsto \sum_{i=1}^{m} \ell \big (f(x_i,w), y_i \big ). \]
Note that in comparison to the formulation given in section \ref{sec:Training}, we removed the constant factor $1/m$ in front of the sum, which has no influence on the minimum. Furthermore we assumed $\lambda = 0$, that is we do not penalize the complexity of $w$. \\

Under the assumption, that all activation functions in $f$ are differentiable, thus $f$ is differentiable itself, the choice of a differentiable loss function $\ell$ enables us to minimize $E$ via gradient descent. In order to analyze this optimization process we define the cost functionals
\[ \L_i: \F \to \R : f_w \mapsto \ell(f_w(x_i),y_i) \]
for $i=1, \dots,m$, such that the empirical error $E$ can be written as
\[ E(w) = \sum_{i=1}^{m} \ell \big (f(x_i,w), y_i \big ) = \sum_{i=1}^{m} \ell \big (f_w(x_i), y_i \big ) = \sum_{i=1}^{m} \L_i(f_w). \]
To investigate the parameter evolution during gradient descent on $E$ we will use the concept of gradient flows.

\begin{definition} \label{def:flow}
The gradient flow according to $E$ is a time dependent function
\[ w: [0, \infty) \to \R^n : t \mapsto w_t, \]
that suffices the ordinary differential equation
\[ \partial_t w_t = - \nabla E(w_t). \]
\end{definition}

Since gradient descent performs parameter updates with regard to the update rule
\[ w_{k+1} \leftarrow w_k - \alpha \cdot \nabla E(w_k), \]
we can let $\alpha \to 0$, to derive the equation
\[ \lim_{\alpha \to 0} \frac{w_{k+1} - w_k}{\alpha} = - \nabla E(w_k). \]
Thus, the gradient flow $w$ covers the parameter dynamics during gradient descent on $E$.

\begin{lemma} \label{lem:flow}
In the case of a single output neural network architecture, the gradient flow according to $E$ suffices the ordinary differential equation
\[ \partial_tw_t = - \nabla_wf(\X, w_t)\tr  \cdot \nabla_{f(\X,w)} \L|_{f_{w_t}}, \]
where 
\[ \nabla_w f(\X, w_t)\tr  \coloneq \Big [ \nabla_wf(x_1,w_t), \dots, \nabla_wf(x_m, w_t) \Big ] \in \R^{n \times m} \]
denotes the gradients of $f$ at $w_t$ on the training data set $\X$ and 
\[ \nabla_{f(\X,w)} \L|_{f_{w_t}} \coloneq \Big [ \nabla \L_1|_{f_{w_t}}(x_1), \dots, \nabla \L_m|_{f_{w_t}}(x_m) \Big ]\tr  \in \R^m \]
denotes the functional gradients of the costs $\L_i$ at $f_{w_t}$ evaluated on $x_i$ for $i=1,\dots,m$.
\end{lemma}

\begin{proof}
By definition \ref{def:flow} we need to show, that
\[ \partial_t w_t = - \nabla E(w_t) = - \nabla_wf(\X, w_t)\tr  \cdot \nabla_{f(\X,w)} \L|_{f_{w_t}}. \]
For arbitrary $w \in \R^n$ we can apply the chain rule to derive the gradient
\[ \nabla E(w) = \sum_{i=1}^{m} \nabla_w \ell \big ( f(x_i,w), y_i \big ) = \sum_{i=1}^{m} \nabla_w f(x_i,w) \cdot \nabla \L_i |_{f_w}(x_i). \]
Using the definitions from the lemma, this is equivalent to
\[ \nabla E(w) = \sum_{i=1}^{m} \nabla_w f(x_i,w) \cdot \nabla \L_i |_{f_w}(x_i) = \nabla_wf(\X, w_t)\tr  \cdot \nabla_{f(\X,w)} \L|_{f_{w_t}}. \]
This completes the proof, since $w$ was chosen arbitrary.
\end{proof}

In the infinite width limit, we can apply theorem \ref{thm:linear} to approximate the neural network architecture $f$ as a linear model
\[ f^\textit{lin}(x, w_t) \approx f(x, w_0) + \nabla_wf(\X,w_0)(w_t-w_0). \]
This is nothing else than a first order Taylor expansion. However, the key observation is, that in the infinite width limit, the error term converges to zero. Based on this, the differential equation in lemma \ref{lem:flow} can be rewritten as
\[ \partial_tw_t = -\nabla_wf(\X,w_0)\tr  \cdot \nabla_{f(\X,w)} \L|_{f_{w_t}}, \]
where the first factor is a constant matrix and the second factor changes over time $t$. \\

At this stage we can explain the main idea of how to reduce the dimensionality of the parameter space. If $\nabla_wf(\X,w_0)$ can be approximated by a low-rank matrix, the training trajectory $(w_k)_{k=0, \dots, K}$ would only depend on this low-rank constant matrix and the functional gradient of $C$, which will enable us to effectively reduce the dimensionality. To clarify this, we apply singular value decomposition on $\nabla_wf(\X,w_0)$ and derive
\[ \nabla_wf(\X,w_0) = U_0 \Sigma_0 V_0\tr , \]
where $U_0 \in \R^{m \times m}$ and $V_0 \in \R^{n \times n}$ are orthogonal and $\Sigma_0 = \text{diag}(\sigma_1, \dots, \sigma_m)$ is positive semidefinite. Using the notation
\[ \tilde{f}: \R^n \to \R^m : w \mapsto \Big [ f(x_1,w), \dots, f(x_m,w) \Big ]\tr , \]
we can apply definition \ref{def:ntk} to derive the neural tangent kernel
\[ \Theta_{w_0} = \sum_{k=1}^{n} \partial_{w_k} \tilde{f}(w_0) \otimes \partial_{w_k} \tilde{f}(w_0) = \nabla_wf(\X,w_0) \cdot \nabla_wf(\X,w_0)\tr  = U_0 \Sigma_0 \Sigma_0\tr  U_0\tr . \]
This is nothing else than the spectral decomposition of $\Theta_0$. By theorem \ref{thm:stieltjes} $\cdots$ . Thus by theorem \ref{thm:eym} we can approximate $\Sigma_0$ by a low-rank matrix $\tilde{\Sigma}_0 = \text{diag}(\sigma_1, \dots, \sigma_d)$, such that
\[ \Sigma_0 \approx \tilde{U}_0 \tilde{\Sigma}_0 \tilde{V}_0\tr , \]
where $\tilde{U}_0 \in \R^{m \times d}$ contains the first $d$ columns of an $m-$dimensional identity matrix and $\tilde{V}_0 \in \R^{n \times d}$ contains the first $d$ columns of an $n-$dimensional identity matrix. Using this approximation we can derive
\[ \nabla_wf(\X,w_0) = U_0 \Sigma_0 V_0\tr  \approx U_0 \tilde{U}_0 \tilde{\Sigma}_0 \tilde{V}_0\tr  V_0\tr . \]
Thus, the dynamics of gradient flow are approximately governed by
\[ \partial_t w_t \approx -\nabla_wf(\X,w_0)\tr  \cdot \nabla_{f(\X,w)} \L|_{f_{w_t}} \approx - V_0 \tilde{V}_0 \Big ( \tilde{\Sigma}_0 \tilde{U}_0\tr  U_0\tr  \cdot \nabla_{f(\X,w)} \L|_{f_{w_t}} \Big ) \]
The clasped part yields the projected gradient in the $d-$dimensional subspace and the variables $V_0 \tilde{V}_0$ establish a projection from the lower-dimensional space back to the $n-$dimensional parameter space. \\

This concludes the theoretical motivation of the subspace extraction. Based on this approach we could effectively train neural networks in the lower-dimensional subspace. However, the previous discussion holds only in the infinite width limit for training in the lazy regime. The rest of this thesis is dedicated to numerical experiments indicating that deep neural networks indeed can be trained in low-dimensional subspaces, although the theoretical conditions do not hold in practice. \\

\textbf{Problem:} Chain rule for functionals. \\
\textbf{Problem:} Reference and existence of gradient flow

\pagebreak
\subsection{Methodology}

The key issue of dimensionality reduction consist of finding a suitable dimension $d \in \N$ and a $d-$dimensional subspace that approximately covers the optimization trajectory. We will address this problem via principal component analysis (PCA), which is based on the concept of orthogonal projections.

\subsubsection{Orthogonal Projections}

The results of this subsection are denoted in section 2.6.1 in \cite{SVD}.

\begin{lemma} \label{lem:orthProjection}
Let $S \subseteq \R^n$ be a subspace. There exists a unique matrix $P \in \R^{n \times n}$, such that
\[ \text{Im}(P) = S \ \wedge \ P^2 = P = P\tr . \]
 The linear map $\tilde{P} : \R^n \to \R^n : x \mapsto Px$ is called the orthogonal projection onto $S$.
\end{lemma}

\begin{proof}
To prove the existence of an orthogonal projection, we let $d \coloneq \text{dim}(S)$ denote the dimension of $S$ and $V := [v_1, \dots, v_d] \in \R^{n \times d}$ denote an orthonormal basis of $S$. We define
\[ P \coloneq VV\tr  \in \R^{n \times n} \]
and observe by the orthonormality of $V$, that $V\tr V = I_d$. Thus $P$ is idempotent, this is
\[ P^2 = VV\tr  VV\tr  = V I_d V\tr  = VV\tr  = P. \]
Furthermore $P$ is symmetric, since 
\[ P\tr  = (VV\tr )\tr  = VV\tr  = P. \]
By the orthonormality of $V$, for arbitrary $s \in S$ there exists $a = (a_1, \dots, a_d)\tr  \in \R^d$ with
\[ s = a_1v_1 + \dots + a_dv_d = Va. \]
Based on this we derive $S \subseteq \text{Im}(P)$ since
\[ Ps = VV\tr s = VV\tr Va = VI_da = Va =s. \]
Additionally $\text{Im}(P) \subseteq S$ due to the fact, that for $y \in \R^n$, we have $\tilde{a} \coloneq V\tr y \in \R^d$, so
\[ Py = VV\tr y = V\tilde{a} \in S. \]
Thus $\text{Im}(P) = S$. This proves the existence of an orthogonal projection $P$ onto $S$. To prove uniqueness of $P$, we should first note that based on $P^2 = P = P\tr $ we have
\[ \forall x,y \in \R^n : (Px)\tr (y-Py) = x\tr P\tr y - x\tr P\tr Py = x\tr Py - x\tr Py = 0. \]
Using $Px \in S$ for every $x \in \R^n$, we can conclude that 
\[ \forall y \in \R^n : y - Py \in S^\perp := \big \{ a \in \R^n \mid \forall b \in S : a\tr b = 0 \big \}. \]
Next we let $P_1$ and $P_2$ denote orthogonal projections onto $S$, then for any $z \in \R^n$ we derive
\[ \begin{split}
\big \| (P_1-P_2)z \big \|_2^2 
&= \big \| P_1z -P_2z \big \|_2^2 \\\
&= z\tr P_1\tr P_1z - z\tr P_2\tr P_1z -  z\tr P_1\tr P_2z + z\tr P_2\tr P_2z \\\
&= z\tr P_1z - z\tr P_2\tr P_1z -  z\tr P_1\tr P_2z + z\tr P_2z,
\end{split} \]
where we have used the property $P_i^2 = P_i = P_i\tr $ for $i=1,2$. Since each term on the right-hand side is a real number, we can transpose each term to derive
\[ \begin{split}
\big \| (P_1-P_2)z \big \|_2^2 
&=  z\tr P_1z - z\tr P_2\tr P_1z -  z\tr P_1\tr P_2z + z\tr P_2z \\\
&= (P_1z)\tr z - (P_1z)\tr P_2z - (P_2z)\tr P_1z + (P_2z)\tr z \\\
&= (P_1z)\tr (z-P_2z) + (P_2z)\tr (z-P_1z).
\end{split} \] 
Using $\text{Im}(P_1) = S = \text{Im}(P_2)$, we observe that 
\[ P_1z \in S \ \wedge \ z-P_2z \in S^\perp \ \wedge \ P_2z \in S \ \wedge \ z - P_1z \in S^\perp. \]
Thus the right-hand side of the equality is zero and by the positive definiteness of the norm we conclude $P_1 - P_2 = 0$. This implies $P_1 = P_2$ showing the uniqueness of $P$.
\end{proof}

In our setting, the $d$-dimensional subspace $S \subseteq \R^n$ is unknown, which is why the following definition will be needed.

\begin{definition}
For $d \in \N$ we define the set of rank-$d$ orthogonal projection matrices
\[ \P_d \coloneq \Big \{ P \in \R^{n \times n} \mid \text{rank}(P) = d \ \wedge \ P^2 = P = P\tr  \Big \}. \]
\end{definition}

Having this background knowledge on orthogonal projections in mind, we can proceed with the theory on principal component analysis.

\subsubsection{Principal Component Analysis} \label{sec:PCA}

The results of this subsection can be found in chapter 15.1 of \cite{PCA}. In the following we fix $d \leq n$ and let $W = [w_1, \dots, w_t] \in \R^{n \times t}$ be a mean-centered data matrix, that is $\sum_{i=1}^{t}w_i=0$. \\

Principal component analysis consists of finding the orthogonal projection matrix $P^* \in \P_d$, that minimizes the reconstruction error
\[ \P_d \to \R : P \mapsto \sum_{i=1}^{t} \big \| Pw_i - w_i \big \|_2^2. \]
Equivalently PCA can be formulated as finding the solution $P^* \in \P_d$, such that
\[ P^* \coloneq \argmin_{P \in \P_d} \big \| PW - W \big \|_F^2, \]
where $\| \cdot \|_F$ denotes the Frobenius norm. Casually speaking, principal component analysis aims to project the original data to a lower-dimensional space while preserving as much information as possible. The following theorem provides a method on solving the PCA problem via singular value decomposition.

\begin{theorem} \label{thm:PCA}
The PCA solution $P^* \in \P_d$ can be decomposed as
\[ P^* = U_dU_d\tr , \]
where $U_d \in \R^{n \times d}$ is the matrix formed by the first $d$ left singular vectors of $W$.
\end{theorem}

\begin{proof}
Let $P \in \P_d$ be arbitrary. By the linearity of the trace, we observe that
\[ \begin{split}
\big \| PW - W \big \|_F^2 
&= \text{Tr} \big [ (PW-W)\tr (PW-W) \big ] \\\
&= \text{Tr} \big [ W\tr  P^2 W - 2 W\tr PW + W\tr W \big ] \\\
&= \text{Tr} \big [ W\tr W\big ] - \text{Tr} \big [W\tr PW \big ],
\end{split} \]
since $P\tr =P=P^2$ by definition. Using that $\text{Tr} \big [ W\tr W\big ]$ is independent of $P$, we derive
\[ \argmin_{P \in \P_d} = \big \| PW-W \big \|_F^2 = \argmax_{P \in \P_d} \text{Tr} \big [ W\tr PW\big ]. \]
By the proof of lemma \ref{lem:orthProjection}, there exists an orthonormal basis $Q = [q_1, \dots, q_d] \in \R^{n \times d}$, such that $P$ can be decomposed as $P=QQ\tr $. Using the invariance of the trace under cyclic permutation and the orthonormality of $Q$, we have
\[ \text{Tr} \big [ W\tr PW \big ] = \text{Tr} \big [ W\tr QQ\tr W \big ] = \text{Tr} \big [ Q\tr WW\tr Q \big ] = \sum_{i=1}^{d} q_i\tr WW\tr q_i. \]
To maximize the rightmost expression we perform SVD on $W = U \Sigma V\tr $, such that
\[ WW\tr  = U \Sigma V\tr  V \Sigma\tr  U\tr  = U \Sigma \Sigma\tr  U\tr . \]
Using the notation $Z = U\tr QQ\tr U$ we derive by the invariance under cyclic permutation, that
\[ \text{Tr} \big [ Q\tr WW\tr Q \big ] = \text{Tr} \big [ Q\tr U \Sigma \Sigma\tr  U\tr Q \big ] = \text{Tr} \big [ U\tr QQ\tr U\Sigma \Sigma\tr  \big ] = \text{Tr} \big [ Z \Sigma \Sigma\tr  \big ]. \]
As $Z$ is orthogonal as a product of three orthogonal matrices we conclude (why orthogonal?)
\[  \text{Tr} \big [ W\tr PW \big ] = \text{Tr} \big [ Z \Sigma \Sigma\tr  ] = \sum_{i=1}^{d} z_{i,i} \sigma_i^2 \leq \sum_{i=1}^{d} \sigma_i^2. \]
The upper bound is attained in the case where $Q = U_d = [u_1, \dots, u_d]$ contains the first $d$ columns of $U$, such that based on corollary \ref{cor:svd} we can use $W\tr u_i = \sigma_iv_i$ to conclude
\[ \text{Tr} \big [ W\tr PW \big ] = \sum_{i=1}^{d} q_i\tr WW\tr q_i = \sum_{i=1}^{d} u_i\tr WW\tr u_i = \sum_{i=1}^{d} u_i\tr W v_i \sigma_i = \sum_{i=1}^{d} \sigma_i^2. \]
This completes the proof.
\end{proof}

Using principal component analysis we can finally construct an effective method for dimensionality reduction.

\subsubsection{Algorithm}

To find the lower-dimensional subspace, that approximately covers the optimization trajectory we first need to sample $t \in \N$ parameter vectors along the optimization trajectory, which can be achieved by training the network with one of the proposed algorithms in section \ref{sec:Training}. \\

The idea of dimensionality reduction consists of applying PCA to the parameters $\{ w_1, \dots, w_t \}$. For this we need to centralize these samples as $\bar{w} \coloneq\sum_{i=1}^{t} w_i$ and define
\[ W \coloneq \big [ w_1 - \bar{w}, \dots, w_t - \bar{w} \big ] \in \R^{n \times t}. \]
This enables us to apply PCA on the mean-centered data matrix $W$. As seen in section \ref{sec:PCA}, for $d \in \N$, the orthogonal projection on the $d-$dimensional subspace, that covers the parameter trajectory best, is induced by
\[ P^* \coloneq \argmin_{P \in \P_d} \big \| PW - W \big \|_F^2. \]
By theorem \ref{thm:PCA} we can construct $P^*$ from the left singular vectors of $W$. For this we apply corollary \ref{cor:svd} to compute the right singular vectors $v_i$ for $i=1,\dots,d$ by the spectral decomposition of $W\tr W \in \R^{t \times t}$. These can then be used to derive the left singular vectors
\[ u_i = \frac{1}{\sigma_i} Wv_i, \quad i=1, \dots, d. \]

Thus we have found an orthonormal basis $U_d = [u_1, \dots, u_d]$ of the $d$-dimensional subspace, that covers the optimization trajectory best. The orthogonal projection onto this subspace is given by
\[ P : \R^n \to \R^n : w \mapsto U_dU_d\tr w. \]

In summary, the algorithm for dimensionality reduction can be notated as follows.

\begin{algorithm}
\caption{Dynamic Linear Dimensionality Reduction (DLDR)} \ \\
\textcolor{white}{$\Big |$}1: Sample parameter trajectory $\{ w_1, \dots, w_t\}$ along the training. \\
\textcolor{white}{$\Big |$}2: Compute the mean $\bar{w} \coloneq\sum_{i=1}^{t} w_i$. \\
\textcolor{white}{$\Big |$}3: Centralize the samples as $W = [w_1-\bar{w}, \dots, w_t - \bar{w}]$. \\
\textcolor{white}{$\Big |$}4: Perform spectral decomposition such that $W\tr W = V \Sigma^2 V^{-1}$.  \\
\textcolor{white}{$\Big |$}5: Choose suitable subspace dimension $d \in \N$. \\
\textcolor{white}{$\Big |$}6: Obtain the $d$ largest eigenvalues $ \big [\sigma_1^2, \dots, \sigma_d^2 \big ]$ with eigenvectors $ \big [v_1, \dots, v_d \big ]$. \\
\textcolor{white}{$\Big |$}7: Determine the singular vectors $u_i = 1/\sigma_iWv_i$ for $i=1, \dots, d$. \\
\textcolor{white}{$\Big |$}8: Return the orthonormal basis $ \big [u_1, \dots, u_d \big ]$.
\end{algorithm}

For complexity ...

\pagebreak
\section{Numerical Experiments}

\pagebreak
\section{Conclusion}

In conclusion...

\pagebreak
\section{Appendix}

\begin{definition}
A real vector space $\H$ is called a Hilbert space, if there exists an inner product $\langle \cdot,\cdot \rangle : \H \times \H \to \R$, such that $H$ is complete with respect to the norm $\| \cdot \| \coloneq \sqrt{\langle \cdot,\cdot \rangle}$.
\end{definition}

\begin{theorem}
Every Hilbert space $\H$ is isometric and isomorphic to its dual space $\H^*$ via
\[ J: \H \to \H^* : x \mapsto \langle x, \cdot \rangle. \]
Therefore each $\hat{x} \in \H^*$ can be represented as $\hat{x} = J(x)$ for some $x \in \H$.
\end{theorem}

\begin{proof}
Let $f, g \in \F$ and $\mu \coloneq J(f)$. By the Cauchy-Schwarz inequality it holds that
\[ \big | \mu[g] \big | = \langle f,g \rangle \leq \big \| f \big \|_{\D_x} \big \| g \big \|_{\D_x} \implies \big \| \mu \big \|_{\F^*} \le \big \| f \big \|_{D_x}. \]
Thus $\mu \in \F^*$ and therefore $J$ is well defined. Furthermore it holds that 
\[ \big | \mu[f] \big | = \langle f,f \rangle = \big \| f \big \|_{\D_x}^2 \implies \big \| \mu \big \|_{\F^*} \ge \big \| f \big \|_{D_x}. \]
The combination of both inequalities yields
\[ \big \| J(f) \big \|_{\F^*} = \big \| \mu \big \|_{\F^*} = \big \| f \big \|_{D_x}, \]
proving that $J$ is indeed an isometry between $\F$ and its dual $\F^*$. Therefore $J$ is injective and it only remains to prove the surjectivity.
\end{proof}

\begin{corollary} \label{cor:descent}
Let $f: \R^n \to \R$ be differentiable with global minimizer $x_* \in \R^n$, such that the gradient $\nabla f$ is Lipschitz continuous with Lipschitz constant $L >0$, then
\[ \forall x \in \R^n : f(x) - f(x_*) \geq \frac{1}{2L} \big \| \nabla f(x) \big \|_2^2. \]
\end{corollary}

\begin{proof}
Let $x \in \R^n$ be arbitrary. Using $y = x - 1/L \cdot \nabla f(x)$ in lemma \ref{lem:descent} yields
\[ \begin{split} 
f(y) 
&\leq f(x) + \big \langle \nabla f(x) , - \frac{1}{L} \nabla f(x) \big \rangle + \frac{L}{2} \big \| \frac{1}{L} \nabla f(x) \big \|_2^2 \\\
&= f(x) - \frac{1}{L} \big \| \nabla f(x) \big \|_2^2 + \frac{1}{2L} \big \| \nabla f(x) \big \|_2^2 \\\
&= f(x) - \frac{1}{2L} \big \| \nabla f(x) \big \|_2^2.
\end{split} \]
Taking the infimum on the lefthand side results in
\[ f(x_*) = \inf_{y \in \R^n} f(y) \leq f(x) - \frac{1}{2L} \big \| \nabla f(x) \big \|_2^2. \]
Rearranging the inequality completes the proof.
\end{proof}

\begin{lemma} \label{lem:coercivity}
Let $f: \R^n \to \R$ be differentiable, such that the gradient $\nabla f$ is Lipschitz continuous with Lipschitz constant $L>0$, then
\[ \forall x,y \in \R^n : \big \langle \nabla f(y) - \nabla f(x),y-x \big \rangle \geq \frac{1}{L} \big \| \nabla f(y) - \nabla f(x) \big \|_2^2. \]
\end{lemma}

\begin{proof}
Let $x,y \in \R^n$ be arbitrary. Define the differentiable functions
\[ g_x: \R^n \to \R : z \mapsto f(z) - \big \langle \nabla f(x), z \big \rangle, \]
\[ g_y: \R^n \to \R : z \mapsto f(z) - \big \langle \nabla f(y), z \big \rangle, \]
then $g_x$ is minimized by $x$ and $g_y$ is minimized by $y$ since the gradients are given by
\[ \nabla g_x(z) = \nabla f(z) - \nabla f(x), \]
\[ \nabla g_y(z) = \nabla f(z) - \nabla f(y). \]
Thus we can apply corollary \ref{cor:descent} to $g_x$ and derive
\[ f(y) - f(x) - \big \langle \nabla f(x) , y-x \big \rangle = g_x(y) - g_x(x) \geq \frac{1}{2L} \big \| \nabla g_x(y) \big \|_2^2. \]
Similarly, applying corollary \ref{cor:descent} to $g_y$ results in
\[ f(x) - f(y) - \big \langle \nabla f(y) , x-y \big \rangle = g_y(x) - g_y(y) \geq \frac{1}{2L} \big \| \nabla g_y(x) \big \|_2^2. \]
Since $ \big \| \nabla g_x(y) \big \|_2^2 = \big \| \nabla f(y) - \nabla f(x) \big \|_2^2 = \big \| \nabla g_y(x) \big \|_2^2$, summation of the inequalities yields
\[ \big \langle \nabla f(y) - \nabla f(x),y-x \big \rangle \geq \frac{1}{L} \big \| \nabla f(y) - \nabla f(x) \big \|_2^2, \]
where we have used $- \big \langle \nabla f(y) , x-y \big \rangle = \big \langle \nabla f(y) , y-x \big \rangle$. This completes the proof.
\end{proof}

\begin{theorem}
Let $f: \R^n \to \R$ be convex and differentiable. Under the assumption that $\nabla f$ is Lipschitz continuous with Lipschitz constant $L>0$, the gradient descent algorithm with $\epsilon = 0$ and $\alpha_k = 1/L$ for every $k \in \N_0$ produces a sequence $(x_k)_{k \in \N_0} \in \R^n$, such that 
\[ \forall k \in \N : f(x_k) - f(x_*) \leq \frac{2L \big \| x_0 - x_* \big \|_2^2 }{k}. \]
\end{theorem}

\begin{proof}
Let $k \in \N$ be arbitrary. Applying lemma \ref{lem:coercivity} to $x_k$ and $x_*$ provides
\[ \big \langle \nabla f(x_k) - \nabla f(x_*), x_k - x_* \big \rangle \ge \frac{1}{L} \big \| \nabla f(x_k) - \nabla f(x_*) \big \|_2^2. \]
Using $\nabla f(x_*) = 0$ and the symmetry of the scalar product, this is equivalent to
\[ \big \langle x_k - x_* ,\nabla f(x_k) \big \rangle \ge \frac{1}{L} \big \| \nabla f(x_k) \big \|_2^2. \]
Together with $x_{k+1} = x_k - 1/L \cdot \nabla f(x_k)$ by gradient descent, this can be used to show
\begin{equation} \begin{split}
\big \| x_{k+1} - x_* \big \|_2^2
&= \big \| x_k - x_* - \frac{1}{L} \nabla f(x_k) \big \|_2^2 \\\
&= \big \| x_k - x_* \big \|_2^2 - 2 \frac{1}{L} \big \langle x_k - x_* , \nabla f(x_k) \big \rangle + \frac{1}{L^2} \big \| \nabla f(x_k) \big \|_2^2 \\\
&\leq \big \| x_k - x_* \big \|_2^2 - \frac{2}{L} \cdot \frac{1}{L} \big \| \nabla f(x_k) \big \|_2^2 + \frac{1}{L^2} \big \| \nabla f(x_k) \big \|_2^2 \\\
& = \big \| x_k - x_* \big \|_2^2 - \frac{1}{L^2} \big \| \nabla f(x_k) \big \|_2^2,
\end{split} \end{equation}
which implies $\big \| x_{k} - x_* \big \|_2^2$ is monotonically decreasing in $k$. Furthermore lemma \ref{lem:descent} yields
\[  f(x_{k+1}) \leq f(x_k) + \big \langle \nabla f(x_k) , x_{k+1} -x_k \big \rangle + \frac{L}{2} \big \| x_{k+1} - x_k \big \|_2^2. \]
By the definition of gradient descent, we can plug in $x_{k+1} - x_k = - 1/L \cdot \nabla f(x_k)$ to derive
\begin{equation} \begin{split} 
f(x_{k+1}) 
&\leq f(x_k) + \big \langle \nabla f(x_k) , - \frac{1}{L} \nabla f(x_k) \big \rangle + \frac{L}{2} \big \| \frac{1}{L} \nabla f(x_k) \big \|_2^2 \\\
&= f(x_k) - \frac{1}{L} \big \| \nabla f(x_k) \big \|_2^2 + \frac{1}{2L} \big \| \nabla f(x_k) \big \|_2^2 \\\
&= f(x_k) - \frac{1}{2L} \big \| \nabla f(x_k) \big \|_2^2.
\end{split} \end{equation}
Hence the gradient descent algorithm with constant step size $\alpha_k = 1/L$ guarantees to make progress unless $\nabla f(x_k) \neq 0$. On top of this we can infer by the convexity of $f$, that
\[ f(x_k) - f(x_*) \leq \big \langle \nabla f(x_k), x_k - x_* \big \rangle. \]
Using the Cauchy-Schwarz inequality and the monotonicity of $\big \| x_{k} - x_* \big \|_2^2$ by (1) yields
\[ f(x_k) - f(x_*) \leq \big \langle \nabla f(x_k), x_k - x_* \big \rangle  \leq \big \| \nabla f(x_k) \big \|_2 \cdot \big \| x_k - x_* \big \|_2 \leq \big \| \nabla f(x_k) \big \|_2 \cdot \big \| x_0 - x_* \big \|_2, \]
which is equivalent to
\begin{equation} \big \| \nabla f(x_k) \big \|_2 \geq \frac{1}{\big \| x_0 - x_* \big \|_2} \big ( f(x_k) - f(x_*) \big ). \end{equation}
Inserting the bound (3) in (2) results in 
\[ f(x_{k+1}) \leq f(x_k) - \frac{1}{2L} \frac{1}{\big \| x_0 - x_* \big \|_2^2} \big ( f(x_k) - f(x_*) \big )^2. \]
Subtracting $f(x_*)$ on both sides and using the notation $\Delta_k \coloneq f(x_k) - f(x_*)$ yields
\[ \Delta_{k+1} \leq \Delta_{k} - \frac{1}{2L} \frac{1}{\big \| x_0 - x_* \big \|_2^2} \Delta_k^2 \ \Leftrightarrow \ \frac{1}{2L} \frac{1}{\big \| x_0 - x_* \big \|_2^2} \Delta_k^2 \leq \Delta_k - \Delta_{k+1}. \]
Since $\Delta_k$ is monotonically decreasing in $k$ by (2), we derive $1 \leq \Delta_k / \Delta_{k+1} $. Expanding the inequality with the positive factor $1/ \Delta_k \Delta_{k+1}$ therefore provides
\[ \frac{1}{2L} \frac{1}{\big \| x_0 - x_* \big \|_2^2} \leq \frac{1}{2L} \frac{1}{\big \| x_0 - x_* \big \|_2^2} \cdot \frac{\Delta_k}{\Delta_{k+1}} \leq  \frac{1}{\Delta_{k+1}} - \frac{1}{\Delta_k}. \]
Summing up both sides from $i=0, \dots, k-1$ yields
\[ k \cdot \frac{1}{2L} \frac{1}{\big \| x_0 - x_* \big \|_2^2} \leq \sum_{i=0}^{k-1} \bigg [ \frac{1}{\Delta_{k+1}} - \frac{1}{\Delta_k} \bigg ] = \frac{1}{\Delta_{k}} - \frac{1}{\Delta_0} \leq \frac{1}{\Delta_{k}}. \]
Finally we can use $\Delta_k = f(x_k) - f(x_*)$ to conclude
\[ \Delta_k = f(x_k) - f(x_*) \leq \frac{2L \big \| x_0 - x_* \big \|_2^2 }{k}. \]
This completes the proof.
\end{proof}

\textbf{Problem:} Maxima in lemma \ref{lem:coercivity}

\pagebreak
\begin{thebibliography}{999}

\bibitem{Paper} Tao Li, Lei Tan, Qinghua Tao, Yipeng Liu, Xiaolin Huang. \textit{Low Dimensional Trajectory Hypothesis is True: DNNs can be Trained in Tiny Subspaces}. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

\bibitem{GD} Augustin Cauchy. \textit{M\'{e}thode g\'{e}n\'{e}rale pour la r\'{e}solution des systemes d'\'{e}quations simultan\'{e}es}. Comptes rendus de l'Acad\'{e}mie des sciences, volume 25, pages 536-538, 1847.

\bibitem{ConvexOptimization} Yurii Nesterov. \textit{Introductory Lectures on Convex Optimization}. Applied Optimization, volume 87, 2004.

\bibitem{BFGS} Richard H. Byrd, Jorge Nocedal, Robert B. Schnabel. \textit{Representations of Quasi-Newton Matrices and their Use in Limited Memory Methods}. Mathematical Programming, volume 60(1), pages 129-156, 1994 

\bibitem{SGD} L\'{e}on Bottou, Frank E. Curtis, Jorge Nocedal. \textit{Optimization Methods for Large-Scale Machine Learning}. SIAM Review, volume 60(2), pages 223-311, 2018.

\bibitem{Adam} Diederik P. Kingma, Jimmy Lei Ba. \textit{Adam: A Method for Stochastic Optimization}. International Conference on Learning Representations (ICLR), 2015.

\bibitem{NTK} Arthur Jacot, Franck Gabriel, Cl\'{e}ment Holger. \textit{Neural Tangent Kernel: Convergence and Generalization in Neural Networks}. Advances in Neural Information Processing Systems, volume 31, pages 8571-8580, 2018.

\bibitem{Functionals} Robert G. Parr, Yang Weitao. \textit{Density-Functional Theory of Atoms and Molecules}. International Series of Monographs on Chemistry Books, volume 16, 1989.

\bibitem{FunctionalAnalysis} Erwin Kreyszig. \textit{Introductory Functional Analysis with Applications}. Wiley Classics Library edition, 1989.

--- Check

\bibitem{Stieltjes} Zhou Fan, Zhichao Wang. \textit{Spectra of the Conjugate Kernel and Neural Tangent Kernel for Linear-Width Neural Networks}. Advances in Neural Information Processing Systems, volume 33, pages 7710-7721, 2020.

\bibitem{Linear} Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington. \textit{Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent}. Journal of Statistical Mechanics: Theory and Experiment. 2020(12):124002, 2020.

\bibitem{SVD} Gene H. Golub, Charles F. Van Loan. \textit{Matrix Computations}. Third edition, 1996.

\bibitem{PCA} Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar. \textit{Foundations of Machine Learning}. Second edition, 2018.

\end{thebibliography}

\end{document}
